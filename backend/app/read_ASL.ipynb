{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3853dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9674f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyPointClassifier(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path='/Users/madswolff/Desktop/CBS/Master/AIML25-shared/backend/app/keypoint_classifier.tflite',\n",
    "        num_threads=1,\n",
    "    ):\n",
    "        self.interpreter = tf.lite.Interpreter(model_path=model_path,\n",
    "                                               num_threads=num_threads)\n",
    "\n",
    "        self.interpreter.allocate_tensors()\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        landmark_list,\n",
    "    ):\n",
    "        input_details_tensor_index = self.input_details[0]['index']\n",
    "        self.interpreter.set_tensor(\n",
    "            input_details_tensor_index,\n",
    "            np.array([landmark_list], dtype=np.float32))\n",
    "        self.interpreter.invoke()\n",
    "\n",
    "        output_details_tensor_index = self.output_details[0]['index']\n",
    "\n",
    "        result = self.interpreter.get_tensor(output_details_tensor_index)\n",
    "\n",
    "        result_index = np.argmax(np.squeeze(result))\n",
    "\n",
    "        return result_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d71e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744667261.056765 4251853 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
      "W0000 00:00:1744667261.065282 4287502 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1744667261.069328 4287502 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not open 'backend/app/data/keypoint_classifier.tflite'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 245\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 245\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 22\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m mp_hands \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mhands\n\u001b[1;32m     14\u001b[0m hands \u001b[38;5;241m=\u001b[39m mp_hands\u001b[38;5;241m.\u001b[39mHands(\n\u001b[1;32m     15\u001b[0m     model_complexity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     16\u001b[0m     static_image_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     min_tracking_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m keypoint_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mKeyPointClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackend/app/data/keypoint_classifier_labels.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     25\u001b[0m     keypoint_classifier_labels \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(f)\n",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m, in \u001b[0;36mKeyPointClassifier.__init__\u001b[0;34m(self, model_path, num_threads)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m      4\u001b[0m     model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackend/app/data/keypoint_classifier.tflite\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     num_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      6\u001b[0m ):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpreter \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInterpreter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_threads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpreter\u001b[38;5;241m.\u001b[39mallocate_tensors()\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpreter\u001b[38;5;241m.\u001b[39mget_input_details()\n",
      "File \u001b[0;32m~/anaconda3/envs/AML_CompVis/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:490\u001b[0m, in \u001b[0;36mInterpreter.__init__\u001b[0;34m(self, model_path, model_content, experimental_delegates, num_threads, experimental_op_resolver_type, experimental_preserve_all_tensors, experimental_disable_delegate_clustering, experimental_default_delegate_latest_features)\u001b[0m\n\u001b[1;32m    484\u001b[0m custom_op_registerers_by_name \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    485\u001b[0m     x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_custom_op_registerers \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    486\u001b[0m ]\n\u001b[1;32m    487\u001b[0m custom_op_registerers_by_func \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    488\u001b[0m     x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_custom_op_registerers \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    489\u001b[0m ]\n\u001b[0;32m--> 490\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpreter \u001b[38;5;241m=\u001b[39m \u001b[43m_interpreter_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateWrapperFromFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_resolver_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_op_registerers_by_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_op_registerers_by_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperimental_preserve_all_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperimental_disable_delegate_clustering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperimental_default_delegate_latest_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpreter:\n\u001b[1;32m    501\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to open \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_path))\n",
      "\u001b[0;31mValueError\u001b[0m: Could not open 'backend/app/data/keypoint_classifier.tflite'."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    cap_device = 0\n",
    "    cap_width = 960\n",
    "    cap_height = 540\n",
    "\n",
    "    use_brect = True\n",
    "\n",
    "    cap = cv.VideoCapture(cap_device)\n",
    "    cap.set(cv.CAP_PROP_FRAME_WIDTH, cap_width)\n",
    "    cap.set(cv.CAP_PROP_FRAME_HEIGHT, cap_height)\n",
    "\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(\n",
    "        model_complexity=0,\n",
    "        static_image_mode=True,\n",
    "        max_num_hands=2,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.7\n",
    "    )\n",
    "\n",
    "    keypoint_classifier = KeyPointClassifier()\n",
    "\n",
    "    with open('/Users/madswolff/Downloads/keypoint_classifier_labels.csv', encoding='utf-8-sig') as f:\n",
    "        keypoint_classifier_labels = csv.reader(f)\n",
    "        keypoint_classifier_labels = [row[0] for row in keypoint_classifier_labels]\n",
    "\n",
    "    mode = 0\n",
    "\n",
    "    while True:\n",
    "        key = cv.waitKey(10)\n",
    "        if key == 27:\n",
    "            break\n",
    "        if key != -1:\n",
    "            print()\n",
    "        number, mode = select_mode(key, mode)\n",
    "\n",
    "        ret, image = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        image = cv.flip(image, 1)\n",
    "        debug_image = copy.deepcopy(image)\n",
    "\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image)\n",
    "        image.flags.writeable = True\n",
    "\n",
    "        if results.multi_hand_landmarks is not None:\n",
    "            for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "                brect = calc_bounding_rect(debug_image, hand_landmarks)\n",
    "                landmark_list = calc_landmark_list(debug_image, hand_landmarks)\n",
    "\n",
    "                pre_processed_landmark_list = pre_process_landmark(landmark_list)\n",
    "\n",
    "                logging_csv(number, mode, pre_processed_landmark_list)\n",
    "\n",
    "                hand_sign_id = keypoint_classifier(pre_processed_landmark_list)\n",
    "\n",
    "                debug_image = draw_bounding_rect(use_brect, debug_image, brect)\n",
    "                debug_image = draw_landmarks(debug_image, landmark_list)\n",
    "                debug_image = draw_info_text(\n",
    "                    debug_image,\n",
    "                    brect,\n",
    "                    handedness,\n",
    "                    keypoint_classifier_labels[hand_sign_id],\n",
    "                    \"\"\n",
    "                )\n",
    "\n",
    "        debug_image = draw_info(debug_image, mode, number)\n",
    "\n",
    "        cv.imshow('Hand Gesture Recognition', debug_image)\n",
    "\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "\n",
    "def logging_csv(number, mode, landmark_list): #Logs the selected number and landmark list to a CSV file if mode is 1.\n",
    " \n",
    "    if mode == 0:\n",
    "        # Mode 0: No logging\n",
    "        pass\n",
    "    elif mode == 1 and (0 <= number <= 25):  # Ensure number corresponds to A-Z (0-25)\n",
    "        csv_path = '/Users/madswolff/Downloads/keypoint.csv'\n",
    "        try:\n",
    "            with open(csv_path, 'a', newline=\"\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([chr(number+97), *landmark_list])\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: CSV path '{csv_path}' not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error while logging to CSV: {e}\")\n",
    "    return\n",
    "\n",
    "def select_mode(key, mode):\n",
    "    number = -1\n",
    "    if ord('a') <= key <= ord('z'):  # A ~ Z\n",
    "        number = key - 97  # Map ASCII A-Z to 0-25\n",
    "    elif key == ord('0'):  # '0' key\n",
    "        mode = 0 # nothing mode\n",
    "    elif key == ord('1'):  # '1' key \n",
    "        mode = 1 # logging mode\n",
    "    return number, mode\n",
    "\n",
    "def pre_process_landmark(landmark_list):\n",
    "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "\n",
    "    # Convert to relative coordinates\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, landmark_point in enumerate(temp_landmark_list):\n",
    "        if index == 0:\n",
    "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
    "\n",
    "        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
    "        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
    "\n",
    "    # Convert to a one-dimensional list\n",
    "    temp_landmark_list = list(\n",
    "        itertools.chain.from_iterable(temp_landmark_list))\n",
    "\n",
    "    # Normalization\n",
    "    max_value = max(list(map(abs, temp_landmark_list)))\n",
    "\n",
    "    def normalize_(n):\n",
    "        return n / max_value\n",
    "\n",
    "    temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
    "\n",
    "    return temp_landmark_list\n",
    "\n",
    "def draw_landmarks(image, landmark_point):\n",
    "    def draw_line_pair(p1, p2):\n",
    "        cv.line(image, tuple(p1), tuple(p2), (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(p1), tuple(p2), (255, 255, 255), 2)\n",
    "\n",
    "    def draw_keypoint(pt, radius):\n",
    "        cv.circle(image, tuple(pt), radius, (255, 255, 255), -1)\n",
    "        cv.circle(image, tuple(pt), radius, (0, 0, 0), 1)\n",
    "\n",
    "    if len(landmark_point) > 0:\n",
    "        # Fingers\n",
    "        finger_connections = [\n",
    "            [2, 3, 4],     # Thumb\n",
    "            [5, 6, 7, 8],  # Index\n",
    "            [9, 10, 11, 12],  # Middle\n",
    "            [13, 14, 15, 16],  # Ring\n",
    "            [17, 18, 19, 20]   # Pinky\n",
    "        ]\n",
    "        for finger in finger_connections:\n",
    "            for i in range(len(finger) - 1):\n",
    "                draw_line_pair(landmark_point[finger[i]], landmark_point[finger[i + 1]])\n",
    "\n",
    "        # Palm\n",
    "        palm_connections = [\n",
    "            (0, 1), (1, 2), (2, 5), (5, 9),\n",
    "            (9, 13), (13, 17), (17, 0)\n",
    "        ]\n",
    "        for start, end in palm_connections:\n",
    "            draw_line_pair(landmark_point[start], landmark_point[end])\n",
    "\n",
    "    # Key points\n",
    "    for index, landmark in enumerate(landmark_point):\n",
    "        if index in [4, 8, 12, 16, 20]:  # Fingertips\n",
    "            draw_keypoint(landmark, 8)\n",
    "        else:\n",
    "            draw_keypoint(landmark, 5)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_bounding_rect(use_brect, image, brect):\n",
    "    if use_brect:\n",
    "        # Outer rectangle\n",
    "        cv.rectangle(image, (brect[0], brect[1]), (brect[2], brect[3]),\n",
    "                     (0, 0, 0), 1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_info_text(image, brect, handedness, hand_sign_text,\n",
    "                   finger_gesture_text):\n",
    "    cv.rectangle(image, (brect[0], brect[1]), (brect[2], brect[1] - 22),\n",
    "                 (0, 0, 0), -1)\n",
    "\n",
    "    info_text = handedness.classification[0].label[0:]\n",
    "    if hand_sign_text != \"\":\n",
    "        info_text = info_text + ':' + hand_sign_text\n",
    "    cv.putText(image, info_text, (brect[0] + 5, brect[1] - 4),\n",
    "               cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv.LINE_AA)\n",
    "\n",
    "    if finger_gesture_text != \"\":\n",
    "        cv.putText(image, \"Finger Gesture:\" + finger_gesture_text, (10, 60),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 0), 4, cv.LINE_AA)\n",
    "        cv.putText(image, \"Finger Gesture:\" + finger_gesture_text, (10, 60),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2,\n",
    "                   cv.LINE_AA)\n",
    "\n",
    "    return image\n",
    "\n",
    "def calc_bounding_rect(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_array = np.empty((0, 2), int)\n",
    "\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "\n",
    "        landmark_point = [np.array((landmark_x, landmark_y))]\n",
    "\n",
    "        landmark_array = np.append(landmark_array, landmark_point, axis=0)\n",
    "\n",
    "    x, y, w, h = cv.boundingRect(landmark_array)\n",
    "\n",
    "    return [x, y, x + w, y + h]\n",
    "\n",
    "def calc_landmark_list(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_point = []\n",
    "\n",
    "    # Keypoint\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "\n",
    "        landmark_point.append([landmark_x, landmark_y])\n",
    "\n",
    "    return landmark_point\n",
    "\n",
    "def draw_info(image, mode, number):\n",
    "    mode_string = ['Logging Key Point', 'Not Logging']\n",
    "    if 1 <= mode <= 2:\n",
    "        cv.putText(image, \"MODE: \" + mode_string[mode - 1], (10, 90),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1,\n",
    "                   cv.LINE_AA)\n",
    "        if 0 <= number <= 25:\n",
    "            cv.putText(image, \"LETTER: \" + str(chr(number+97)), (10, 110),\n",
    "                       cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1,\n",
    "                       cv.LINE_AA)\n",
    "    return image\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML_CompVis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
