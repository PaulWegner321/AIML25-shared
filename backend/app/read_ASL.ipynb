{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f7ba956",
   "metadata": {},
   "source": [
    "**INTRO TO HOW YOU USE THIS:**\n",
    "1. WHEN YOU RUN THE SCRIPT IT WILL OPEN AN EXTERNAL CAMERA WINDOW â€“ THIS IS WHAT WE ARE USING TO LOG THE DATA.\n",
    "2. IF YOU WISH TO ENTER LOGGING MODE (WHERE YOU CAN COLLECT THE DATAPOINTS FOR THE HANDSIGN YOU'RE MAKING) PRESS \"1\"\n",
    "3. MAKE THE HAND SIGN YOU WANT AND PRESS THE CORRESPONDING KEY ON THE KEYBOARD AND IT WILL BE LOGGED (A MESSAGE WILL APPEAR BRIEFLY INDICATING WHAT LETTER YOU PRESSED)\n",
    "4. TO EXIT LOGGING MODE, PRESS \"0\"\n",
    "5. TO EXIT THE PROGRAM ALTOGETHER, PRESS \"ESC\"\n",
    "\n",
    "_A note on how the data is structured in the .csv: Each row represents one logged hand signal. That is, when you have the program open in logging mode and click a possible button. The following 42 columns are the respective X and Y 'coordinates' of each segment of the hand. The first two columns are always 0,0 as these are the palm node from which the relative position of all the other segments is calculated. While this may seem odd, it makes a lot more sense to calculate the finger position relatively as opposed to where they appear pixel wise in the image, as this is prone to change and would make the model very rigid and overiftting._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb3cb0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow, mediapipe, opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3853dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d9674f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyPointClassifier(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path='./data/keypoint_classifier.tflite',\n",
    "        num_threads=1,\n",
    "    ):\n",
    "        self.interpreter = tf.lite.Interpreter(model_path=model_path,\n",
    "                                               num_threads=num_threads)\n",
    "\n",
    "        self.interpreter.allocate_tensors()\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        landmark_list,\n",
    "    ):\n",
    "        input_details_tensor_index = self.input_details[0]['index']\n",
    "        self.interpreter.set_tensor(\n",
    "            input_details_tensor_index,\n",
    "            np.array([landmark_list], dtype=np.float32))\n",
    "        self.interpreter.invoke()\n",
    "\n",
    "        output_details_tensor_index = self.output_details[0]['index']\n",
    "\n",
    "        result = self.interpreter.get_tensor(output_details_tensor_index)\n",
    "\n",
    "        result_index = np.argmax(np.squeeze(result))\n",
    "\n",
    "        return result_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "82d71e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744667875.017407 4251853 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
      "W0000 00:00:1744667875.024012 4298759 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1744667875.029027 4298759 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    cap_device = 0\n",
    "    cap_width = 960\n",
    "    cap_height = 540\n",
    "\n",
    "    use_brect = True\n",
    "\n",
    "    cap = cv.VideoCapture(cap_device)\n",
    "    cap.set(cv.CAP_PROP_FRAME_WIDTH, cap_width)\n",
    "    cap.set(cv.CAP_PROP_FRAME_HEIGHT, cap_height)\n",
    "\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(\n",
    "        model_complexity=0,\n",
    "        static_image_mode=True,\n",
    "        max_num_hands=2,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.7\n",
    "    )\n",
    "\n",
    "    keypoint_classifier = KeyPointClassifier()\n",
    "\n",
    "    with open('./data/keypoint_classifier_labels.csv', encoding='utf-8-sig') as f:\n",
    "        keypoint_classifier_labels = csv.reader(f)\n",
    "        keypoint_classifier_labels = [row[0] for row in keypoint_classifier_labels]\n",
    "\n",
    "    mode = 0\n",
    "\n",
    "    while True:\n",
    "        key = cv.waitKey(10)\n",
    "        if key == 27:\n",
    "            break\n",
    "        if key != -1:\n",
    "            print()\n",
    "        number, mode = select_mode(key, mode)\n",
    "\n",
    "        ret, image = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        image = cv.flip(image, 1)\n",
    "        debug_image = copy.deepcopy(image)\n",
    "\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image)\n",
    "        image.flags.writeable = True\n",
    "\n",
    "        if results.multi_hand_landmarks is not None:\n",
    "            for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "                brect = calc_bounding_rect(debug_image, hand_landmarks)\n",
    "                landmark_list = calc_landmark_list(debug_image, hand_landmarks)\n",
    "\n",
    "                pre_processed_landmark_list = pre_process_landmark(landmark_list)\n",
    "\n",
    "                logging_csv(number, mode, pre_processed_landmark_list)\n",
    "\n",
    "                hand_sign_id = keypoint_classifier(pre_processed_landmark_list)\n",
    "\n",
    "                debug_image = draw_bounding_rect(use_brect, debug_image, brect)\n",
    "                debug_image = draw_landmarks(debug_image, landmark_list)\n",
    "                debug_image = draw_info_text(\n",
    "                    debug_image,\n",
    "                    brect,\n",
    "                    handedness,\n",
    "                    keypoint_classifier_labels[hand_sign_id],\n",
    "                    \"\"\n",
    "                )\n",
    "\n",
    "        debug_image = draw_info(debug_image, mode, number)\n",
    "\n",
    "        cv.imshow('Hand Gesture Recognition', debug_image)\n",
    "\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "\n",
    "def logging_csv(number, mode, landmark_list): #Logs the selected number and landmark list to a CSV file if mode is 1.\n",
    " \n",
    "    if mode == 0:\n",
    "        # Mode 0: No logging\n",
    "        pass\n",
    "    elif mode == 1 and (0 <= number <= 25):  # Ensure number corresponds to A-Z (0-25)\n",
    "        csv_path = './data/keypoint.csv'\n",
    "        try:\n",
    "            with open(csv_path, 'a', newline=\"\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([chr(number+97), *landmark_list])\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: CSV path '{csv_path}' not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error while logging to CSV: {e}\")\n",
    "    return\n",
    "\n",
    "def select_mode(key, mode):\n",
    "    number = -1\n",
    "    if ord('a') <= key <= ord('z'):  # A ~ Z\n",
    "        number = key - 97  # Map ASCII A-Z to 0-25\n",
    "    elif key == ord('0'):  # '0' key\n",
    "        mode = 0 # nothing mode\n",
    "    elif key == ord('1'):  # '1' key \n",
    "        mode = 1 # logging mode\n",
    "    return number, mode\n",
    "\n",
    "def pre_process_landmark(landmark_list):\n",
    "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "\n",
    "    # Convert to relative coordinates\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, landmark_point in enumerate(temp_landmark_list):\n",
    "        if index == 0:\n",
    "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
    "\n",
    "        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
    "        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
    "\n",
    "    # Convert to a one-dimensional list\n",
    "    temp_landmark_list = list(\n",
    "        itertools.chain.from_iterable(temp_landmark_list))\n",
    "\n",
    "    # Normalization\n",
    "    max_value = max(list(map(abs, temp_landmark_list)))\n",
    "\n",
    "    def normalize_(n):\n",
    "        return n / max_value\n",
    "\n",
    "    temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
    "\n",
    "    return temp_landmark_list\n",
    "\n",
    "def draw_landmarks(image, landmark_point):\n",
    "    def draw_line_pair(p1, p2):\n",
    "        cv.line(image, tuple(p1), tuple(p2), (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(p1), tuple(p2), (255, 255, 255), 2)\n",
    "\n",
    "    def draw_keypoint(pt, radius):\n",
    "        cv.circle(image, tuple(pt), radius, (255, 255, 255), -1)\n",
    "        cv.circle(image, tuple(pt), radius, (0, 0, 0), 1)\n",
    "\n",
    "    if len(landmark_point) > 0:\n",
    "        # Fingers\n",
    "        finger_connections = [\n",
    "            [2, 3, 4],     # Thumb\n",
    "            [5, 6, 7, 8],  # Index\n",
    "            [9, 10, 11, 12],  # Middle\n",
    "            [13, 14, 15, 16],  # Ring\n",
    "            [17, 18, 19, 20]   # Pinky\n",
    "        ]\n",
    "        for finger in finger_connections:\n",
    "            for i in range(len(finger) - 1):\n",
    "                draw_line_pair(landmark_point[finger[i]], landmark_point[finger[i + 1]])\n",
    "\n",
    "        # Palm\n",
    "        palm_connections = [\n",
    "            (0, 1), (1, 2), (2, 5), (5, 9),\n",
    "            (9, 13), (13, 17), (17, 0)\n",
    "        ]\n",
    "        for start, end in palm_connections:\n",
    "            draw_line_pair(landmark_point[start], landmark_point[end])\n",
    "\n",
    "    # Key points\n",
    "    for index, landmark in enumerate(landmark_point):\n",
    "        if index in [4, 8, 12, 16, 20]:  # Fingertips\n",
    "            draw_keypoint(landmark, 8)\n",
    "        else:\n",
    "            draw_keypoint(landmark, 5)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_bounding_rect(use_brect, image, brect):\n",
    "    if use_brect:\n",
    "        # Outer rectangle\n",
    "        cv.rectangle(image, (brect[0], brect[1]), (brect[2], brect[3]),\n",
    "                     (0, 0, 0), 1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_info_text(image, brect, handedness, hand_sign_text,\n",
    "                   finger_gesture_text):\n",
    "    cv.rectangle(image, (brect[0], brect[1]), (brect[2], brect[1] - 22),\n",
    "                 (0, 0, 0), -1)\n",
    "\n",
    "    info_text = handedness.classification[0].label[0:]\n",
    "    if hand_sign_text != \"\":\n",
    "        info_text = info_text + ':' + hand_sign_text\n",
    "    cv.putText(image, info_text, (brect[0] + 5, brect[1] - 4),\n",
    "               cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv.LINE_AA)\n",
    "\n",
    "    if finger_gesture_text != \"\":\n",
    "        cv.putText(image, \"Finger Gesture:\" + finger_gesture_text, (10, 60),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 0), 4, cv.LINE_AA)\n",
    "        cv.putText(image, \"Finger Gesture:\" + finger_gesture_text, (10, 60),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2,\n",
    "                   cv.LINE_AA)\n",
    "\n",
    "    return image\n",
    "\n",
    "def calc_bounding_rect(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_array = np.empty((0, 2), int)\n",
    "\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "\n",
    "        landmark_point = [np.array((landmark_x, landmark_y))]\n",
    "\n",
    "        landmark_array = np.append(landmark_array, landmark_point, axis=0)\n",
    "\n",
    "    x, y, w, h = cv.boundingRect(landmark_array)\n",
    "\n",
    "    return [x, y, x + w, y + h]\n",
    "\n",
    "def calc_landmark_list(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_point = []\n",
    "\n",
    "    # Keypoint\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "\n",
    "        landmark_point.append([landmark_x, landmark_y])\n",
    "\n",
    "    return landmark_point\n",
    "\n",
    "def draw_info(image, mode, number):\n",
    "    mode_string = ['Logging Key Point', 'Not Logging']\n",
    "    if 1 <= mode <= 2:\n",
    "        cv.putText(image, \"MODE: \" + mode_string[mode - 1], (10, 90),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1,\n",
    "                   cv.LINE_AA)\n",
    "        if 0 <= number <= 25:\n",
    "            cv.putText(image, \"LETTER: \" + str(chr(number+97)), (10, 110),\n",
    "                       cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1,\n",
    "                       cv.LINE_AA)\n",
    "    return image\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML_CompVis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
