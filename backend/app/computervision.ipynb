{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37687f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision  \n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F  \n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3434412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations (rescale = normalize in PyTorch)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),                # Ensure grayscale (1 channel)\n",
    "    #transforms.Resize((96, 96)),    \n",
    "    transforms.Resize((48,48)),       # Resize images\n",
    "    transforms.ToTensor(),                 # Converts to [0,1] float tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))   # Optional: normalize to [-1,1]\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# Load training dataset\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root='/Users/madswolff/Downloads/asl_alphabet_train',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "#laod test dataset\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    root='/Users/madswolff/Downloads/asl_alphabet_test',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Class names (if you want them)\n",
    "train_class_names = train_dataset.classes\n",
    "test_class_names = test_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "befb616c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset ImageFolder\n",
       "     Number of datapoints: 78000\n",
       "     Root location: /Users/madswolff/Downloads/asl_alphabet_train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Grayscale(num_output_channels=1)\n",
       "                Resize(size=(48, 48), interpolation=bilinear, max_size=None, antialias=True)\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.5,), std=(0.5,))\n",
       "            ),\n",
       " Dataset ImageFolder\n",
       "     Number of datapoints: 1815\n",
       "     Root location: /Users/madswolff/Downloads/asl_alphabet_test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Grayscale(num_output_channels=1)\n",
       "                Resize(size=(48, 48), interpolation=bilinear, max_size=None, antialias=True)\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.5,), std=(0.5,))\n",
       "            ))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e63f09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class ASL_CNN(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n\\n        # Define the convolutional layers\\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # Input 1 channel, output 32 channels\\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # Input 32 channels, output 64 channels\\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # Input 64 channels, output 128 channels\\n        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # Input 128 channels, output 128 channels\\n        self.pool = nn.MaxPool2d(kernel_size=2)  # 2x2 Max pooling\\n        self.relu = nn.ReLU()  # ReLU activation\\n        \\n        # Flatten layer to convert 2D image data into 1D feature vector\\n        self.flatten = nn.Flatten()\\n\\n        # Fully connected layers\\n        self.fc1 = nn.Linear(128 * 6 * 6, 512)  # Adjusted input size after feature maps from conv layers\\n        self.fc2 = nn.Linear(512, 26)  # Output layer for 26 classes (ASL alphabet)\\n\\n    def forward(self, x):\\n        # Apply convolutional layers with ReLU activation and pooling\\n        x = self.pool(self.relu(self.conv1(x)))\\n        x = self.pool(self.relu(self.conv2(x)))\\n        x = self.pool(self.relu(self.conv3(x)))\\n        x = self.pool(self.relu(self.conv4(x)))  # Fourth convolution and pooling layer\\n\\n        # Flatten the feature maps into a 1D vector\\n        x = self.flatten(x)\\n\\n        # Apply fully connected layers\\n        x = self.relu(self.fc1(x))\\n        x = self.fc2(x)  # Output layer for classification\\n\\n        return x'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class ASL_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # Input 1 channel, output 32 channels\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # Input 32 channels, output 64 channels\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # Input 64 channels, output 128 channels\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # Input 128 channels, output 128 channels\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)  # 2x2 Max pooling\n",
    "        self.relu = nn.ReLU()  # ReLU activation\n",
    "        \n",
    "        # Flatten layer to convert 2D image data into 1D feature vector\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 6 * 6, 512)  # Adjusted input size after feature maps from conv layers\n",
    "        self.fc2 = nn.Linear(512, 26)  # Output layer for 26 classes (ASL alphabet)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers with ReLU activation and pooling\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = self.pool(self.relu(self.conv4(x)))  # Fourth convolution and pooling layer\n",
    "\n",
    "        # Flatten the feature maps into a 1D vector\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Apply fully connected layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # Output layer for classification\n",
    "\n",
    "        return x'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99e0a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASL_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # Input 1 channel, output 32 channels\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # Input 32 channels, output 64 channels\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # Input 64 channels, output 128 channels\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # Input 128 channels, output 128 channels\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)  # 2x2 Max pooling\n",
    "        self.relu = nn.ReLU()  # ReLU activation\n",
    "        \n",
    "        # Flatten layer to convert 2D image data into 1D feature vector\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Fully connected layers\n",
    "        # Adjusted input size after feature maps from conv layers\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 512)  # Adjusted for the new size (128 channels * 3x3 feature map)\n",
    "        self.fc2 = nn.Linear(512, 26)  # Output layer for 26 classes (ASL alphabet)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers with ReLU activation and pooling\n",
    "        x = self.pool(self.relu(self.conv1(x)))  # 48x48 -> 24x24\n",
    "        x = self.pool(self.relu(self.conv2(x)))  # 24x24 -> 12x12\n",
    "        x = self.pool(self.relu(self.conv3(x)))  # 12x12 -> 6x6\n",
    "        x = self.pool(self.relu(self.conv4(x)))  # 6x6 -> 3x3\n",
    "\n",
    "        # Flatten the feature maps into a 1D vector\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Apply fully connected layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # Output layer for classification\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "95ce1c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASL_CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=1152, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=26, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ASL_CNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f6c0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ASL_CNN().to(device)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990a065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 610/610 [30:47<00:00,  3.03s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.8351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 610/610 [03:38<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.1177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 610/610 [03:35<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.0622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 610/610 [03:39<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.0422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 610/610 [03:33<00:00,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.0312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 5  # We will run for 5 epochs\n",
    "model.train()  # Set model to training mode\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0  # Initialize running loss for this epoch\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{EPOCHS}\"):\n",
    "        images, labels = batch  # Unpack images and labels\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)  # Pass images through the model\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Update the running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {running_loss / len(train_loader):.4f}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd7aada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 15/15 [00:02<00:00,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.00      0.00      0.00        70\n",
      "           B       0.00      0.00      0.00        70\n",
      "           C       0.00      0.00      0.00        70\n",
      "           D       0.00      0.00      0.00        70\n",
      "           E       0.07      0.71      0.13        70\n",
      "           F       0.00      0.00      0.00        70\n",
      "           G       0.03      0.09      0.05        70\n",
      "           H       0.00      0.00      0.00        70\n",
      "           I       0.00      0.00      0.00        70\n",
      "           J       0.00      0.00      0.00        70\n",
      "           K       0.00      0.00      0.00        70\n",
      "           L       0.00      0.00      0.00        70\n",
      "           M       0.05      0.03      0.04        70\n",
      "           N       0.00      0.00      0.00        70\n",
      "           O       0.00      0.00      0.00        70\n",
      "           P       0.00      0.00      0.00        70\n",
      "           Q       0.00      0.00      0.00        70\n",
      "           R       0.00      0.00      0.00        70\n",
      "           S       1.00      0.07      0.13        70\n",
      "           T       0.00      0.00      0.00        65\n",
      "           U       0.00      0.00      0.00        70\n",
      "           V       0.05      0.07      0.06        70\n",
      "           W       0.66      0.59      0.62        70\n",
      "           X       0.03      0.31      0.06        70\n",
      "           Y       0.00      0.00      0.00        70\n",
      "           Z       0.00      0.00      0.00        70\n",
      "\n",
      "    accuracy                           0.07      1815\n",
      "   macro avg       0.07      0.07      0.04      1815\n",
      "weighted avg       0.07      0.07      0.04      1815\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/madswolff/anaconda3/envs/aiml25-ma1/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/madswolff/anaconda3/envs/aiml25-ma1/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/madswolff/anaconda3/envs/aiml25-ma1/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "'''# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Lists to store true and predicted labels\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Disable gradient calculation during evaluation (for efficiency)\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images, labels = batch\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        outputs = model(images)  \n",
    "        \n",
    "        # Get the predicted class by selecting the index with the highest score\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Store the true and predicted labels\n",
    "        y_true.extend(labels.cpu().numpy())  # Move to CPU and convert to numpy array\n",
    "        y_pred.extend(predictions.cpu().numpy())  # Move to CPU and convert to numpy array\n",
    "\n",
    "# Print the classification report\n",
    "target_names = [chr(i) for i in range(65, 91)]  # ASL alphabet 'A' to 'Z'\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95f0566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGZCAYAAABmNy2oAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHT1JREFUeJzt3VuMnWX1x/E1pbPn1JkemJZpp5nSjg61IKnRWJUYtb0QSGMiIRRvTPSGhFQSI0ZN1HDQK6MxMSH1wuIFJhATjMcLRQlXkBTiRRUwFNu0I0w5tEzb6cyezjj/C/6utCnv+r3utR/2VL6fhAu7+rz73e8+LDf81vN2LS0tLRkAAGa2otMnAABYPmgKAABHUwAAOJoCAMDRFAAAjqYAAHA0BQCAoykAABxNAQDgaApYFn7+859bV1eXPfvss205XldXl+3fv78tx7r4mPfdd1/qGP/85z9t//79NjExYX19fdbf32/XX3+9ffvb37Z//etf7TlRIGFlp08AeK/43e9+Z3feeacNDw/b/v377UMf+pB1dXXZ4cOH7eDBg/b73//e/vrXv3b6NPEeR1MA3gVHjx61O++80yYmJuzJJ5+01atXe2337t12zz332K9+9asOniHwNv71Ea4Yc3Nz9rWvfc127txpq1evtnXr1tnHP/5x+/Wvf1255qc//alNTExYT0+P7dixwx599NHL/s7U1JTdddddtnnzZms0GrZ161a7//77bWFhoW3n/qMf/chmZmbsoYceuqQh/EdXV5fddtttbXs8oFX8UsAVo9ls2qlTp+zee++10dFRm5+ftyeeeMJuu+02e/jhh+2LX/ziJX//N7/5jT355JP2wAMP2MDAgD300EP2hS98wVauXGm33367mb3dED760Y/aihUr7Lvf/a6Nj4/b008/bd/73vfs2LFj9vDDD4fndO2115qZ2bFjx8K/98c//tGuueYa+9jHPtby8wfeDTQFXDFWr159yZf04uKi7dmzx06fPm0//vGPL2sKb7zxhh06dMiuueYaMzO79dZb7YYbbrBvfetb3hTuu+8+O336tP3973+3sbExMzPbs2eP9fX12b333mtf//rXbceOHZXntHJlvY/Q8ePHbefOnf/N0wU6gn99hCvKL3/5S7vpppts1apVtnLlSuvu7raf/exn9sILL1z2d/fs2eMNwczsqquusn379tmRI0dscnLSzN7+j7+f+cxnbNOmTbawsOD/3HLLLWZm9tRTT4Xnc+TIETty5EgbnyHQWTQFXDEef/xxu+OOO2x0dNQeeeQRe/rpp+3QoUP25S9/2ebm5i77+yMjI5V/9uabb5qZ2cmTJ+23v/2tdXd3X/LP9ddfb2Zv/9poh7GxMTt69GhbjgWUxL8+whXjkUcesa1bt9pjjz1mXV1d/ufNZvMd//7U1FTln1199dVmZjY8PGw33nijff/733/HY2zatCl72mZm9tnPftZ+8pOf2DPPPMN/V8Cyxi8FXDG6urqs0Whc0hCmpqYq00d//vOf7eTJk/6/FxcX7bHHHrPx8XHbvHmzmZnt3bvX/va3v9n4+Lh95CMfueyfdjWFr371qzYwMGB33323TU9PX1ZfWloikoplgV8KWFb+8pe/vGOS59Zbb7W9e/fa448/bnfffbfdfvvtduLECXvwwQdt48aN9tJLL122Znh42Hbv3m3f+c53PH304osvXhJLfeCBB+xPf/qTfeITn7B77rnHrrvuOpubm7Njx47ZH/7wBztw4IA3kHfyvve9z8xM/neFrVu32qOPPmr79u2znTt3+vCamdnzzz9vBw8etKWlJfv85z9f5zIBxdAUsKx84xvfeMc/P3r0qH3pS1+y1157zQ4cOGAHDx60bdu22Te/+U2bnJy0+++//7I1n/vc53wLiePHj9v4+Lj94he/sH379vnf2bhxoz377LP24IMP2g9+8AObnJy0wcFB27p1q9188822du3a8Hz/m1mGvXv32uHDh+2HP/yhHThwwE6cOGErVqzwx/rKV75S+1hAKV1LS0tLnT4JAMDywH9TAAA4mgIAwNEUAACOpgAAcDQFAICjKQAAXO05hQ0bNoT1KK+tUq/ZeknRY188Wfvfrs08bvbYeG9ZsSL+/35XXXVVZU29x5XM+1Q9tnpeqp4RnVt0PdVaM7Pu7u6wvn79+rAeOXTokPw7/FIAADiaAgDA0RQAAI6mAABwNAUAgKMpAAAcTQEA4GrPKajsbJRHXlxcDNf++9//DutqffTYJWckVN5YPa9WH7d0veQMRPbY2dx8RJ1bycdWMpl7lZuPPtvZ93jm87NyZfz1pL4XovUlZxiU/v7+sK7u9rdmzZrK2vnz51s5pUvwSwEA4GgKAABHUwAAOJoCAMDRFAAAjqYAAHC1I6mNRiOsRzErFf9S0bILFy6E9SgWNz8/n3rsaEvwTFTWLD7vTm43vpwfOyMbOc3EK5XMehU5zWzlnH09osfOnrf6/EXPS13vTFx2aGgoXDs6OhrWVWQ1E3Wvg18KAABHUwAAOJoCAMDRFAAAjqYAAHA0BQCAoykAAFztOYV2bMlaRc0xqHqUGe7p6QnXqix0Rma+QmWwm81mWO/kduPLecvwTh27k9tuq/OO3ivqvDOfzexnT23nn5lTUMdev359ZW3Dhg3hWvW81esVrW/HluD8UgAAOJoCAMDRFAAAjqYAAHA0BQCAoykAABxNAQDgas8pRHljs9ye7CrXru6JENWzj53JUmf2ZO/t7Q3X9vX1hXV13tGcg5pxUPXomkb3p6hTz97DIrO25L0elMxciXqPR9n27AxRyXs1qMeOPgPq8zU8PBzWozmF0qLr1o57LfBLAQDgaAoAAEdTAAA4mgIAwNEUAACOpgAAcDQFAICrPaewbt26sL5q1arKmpozUPdqUOuj7Lq6p4HKxbf6uHXq0WPPzs62dE51RRluNeOg6o1Go7KmZjcy18wszmmrDHd2RiKSnSWIrls2m555LyjR81bHVu8VtT76ThoZGQnXDg4OhvXommfvaZC594aaJ6uDXwoAAEdTAAA4mgIAwNEUAACOpgAAcDQFAICrnV/avHlzWN+yZUtlTUVKVVxPRVaj+KZaOzc31/Kx1dqZmZmwHkUcS8cno2uuYrzq3EpuX52J+2WitGa556WuWeaaltxOXMUjM7FR9Vp2d3eHdRUbjWKnauvszPuwdCQ1uqaZOOt/8EsBAOBoCgAAR1MAADiaAgDA0RQAAI6mAABwNAUAgKs9p9BsNluuq0y9yiMPDAy0XM9mhjOzBGo+I7pmagYiM19hZnbu3LnKmjrvzGOrTH12BiJan83ztyMD3uqxo3r2PZ6R2VpbbfM8PDycqkfHV+8z9byiY2ffJ518Pc34pQAAuAhNAQDgaAoAAEdTAAA4mgIAwNEUAACOpgAAcLXnFBSV+42o/fsz2fSsKK+scu8q6xztB69mM1TGO0PNpKg5hqgezUeY5eYrzOIZCjVfoZ5X9HqrWZxOvofVsaNcfDZzH923YHR0NFyr7peQOTc1C6Dq0WOr81LHVt8b0fHb8b3ALwUAgKMpAAAcTQEA4GgKAABHUwAAOJoCAMC9K5FUFcFSkVS1tXYU0cpuhxytV1E/FVOMros6bxWvzMT11GOr1zOKIUY1s3wEMnpNMluZm8VxWLVWvV4qaht9RtTzUu/DTBy2v78/rG/cuLGypmLXKvKt4peZ2GjJuKuS2Y68Hdu780sBAOBoCgAAR1MAADiaAgDA0RQAAI6mAABwNAUAgKs9p6Cyt9Esgcq9Z7aKVetV1lk9ryjDnTmv5Uw9L5V7j665ei8oan1Uz26XHG3lPDQ01PJ5melZgcycwszMTFhXMxQRNWsQfS+o56zmENTnK6qrtZmts7Of+8zW2uq9UOvx00cAAPzPoCkAABxNAQDgaAoAAEdTAAA4mgIAwNEUAACu9pxCyT2+1R77mXsiNBqNcK3KSkd1dU0y2fTsPQ0yzys7SxBdF3VsNQORObfsfQXakQGvknmPZz9fUV3NCqj3YTSzos47cx8Vs9z9SpTMDMRyxy8FAICjKQAAHE0BAOBoCgAAR1MAADiaAgDA1Y6kKpkYVumtZjMy0U0VcYwidZlIaR3R+mazmTp25nkpJbfOVpHVKN6sziva+tpMRz8zz0uJ1me3mI7qPT094VoVOc1urZ157MzaTJRW1dsRh+WXAgDA0RQAAI6mAABwNAUAgKMpAAAcTQEA4GgKAABXe04hk63N5nYzj53NxWcy4GptJs+f2WrZLM7kZ7ZDNtOZ/Eh26+yorq6pyr1Hz1sdW+XHM6+3WpvZPj47AxE9b3Xs7LbdmVkD9diZx1VbgqvnVXrbbn4pAAAcTQEA4GgKAABHUwAAOJoCAMDRFAAAjqYAAHBtm1PIyGbuS+asM8fOZtdLinLY6vXI7GOvronKcKv1mVmCzOulrpma7VD16NyyMxDz8/NhPaLuiVByTkHJrFfXNPpOUu/h7D0qosdux/c0vxQAAI6mAABwNAUAgKMpAAAcTQEA4GgKAABHUwAAuFwQ+CLZeYBOPXZmbTYTnMk6Kyo3H2XXs/dyyMjm/aPrlp0VUPVIdu//6Lqoa6buURGdW8mZlaySn3t13tnPZyRzfxnmFAAAbUVTAAA4mgIAwNEUAACOpgAAcDQFAIBr29bZJbfWzsTHVLxSiY6dvSZR3K/klt9m8fNSMUN17Mx7QUUgM9uVq5ihim5Gz0vFVTNbMZvFzyv72JlIqjrvzPs4u3V29LzVNclEabPXJPNeaUcEmF8KAABHUwAAOJoCAMDRFAAAjqYAAHA0BQCAoykAAFztILDK1kaZ4pIzDGZxNleddzYXH8lkz7NUxjs6t+w1aTQaLR87u213tD6buY+uqZoVUM9rfn4+rJfcgjpzzdR7Iapn5xAyc0KZ2Y3ssbMzK1E9e03N+KUAALgITQEA4GgKAABHUwAAOJoCAMDRFAAAjqYAAHBtu59CyTx/Zs6hk/d5UNn0aH32vDOvR/Z+CVFWWt2zIJvHj3L16nllZg1UPvzChQthveRMizq2ek0youuSmXEwK3tPBCVzP5LsZztzj5c6+KUAAHA0BQCAoykAABxNAQDgaAoAAEdTAAC4/D6r/09tsRvJxCezSsbWVDQtEzPMbO2r6tktpjNb+2beR2q9OraKpEb1bIxXPXYUac1GOzPXPPNe6O7uTh07c97Z7auja5o9thJdt6GhodSxzfilAAC4CE0BAOBoCgAAR1MAADiaAgDA0RQAAI6mAABwbds6O6pnc9RKlBHPHjuTR1Y56ujYam02z595vUrKZrwz10Xl5qMtptWcgnpeak4hWq+25Vaic1fXMzNrk5WdDYlkZygi6r3Q09MT1icmJlpeWwe/FAAAjqYAAHA0BQCAoykAABxNAQDgaAoAAEdTAAC4ts0pZLLtJXPxJecUlJL3NMjmw0vOKWQy3JlrpurZ+0RE+fLs6xXNQJjlrqk6tyjb3mw2w7Xz8/NhPXreMzMzLa8107Md0fNWef6NGzeG9ei+IOqeIWvXrg3rN954Y1jftGlTZW1ycjJcWwe/FAAAjqYAAHA0BQCAoykAABxNAQDgaAoAAEdTAAC4ts0pRNncbKZerY9y9dnce0TlpFXeP8q9Z56zWf5+C5GSr4eSmSXIPnYkO9uh7okQHV/l4tW5nT9/vrKWmUNQdfX5mZ2dDetqtiOq9/X1hWsHBwfD+rp16yprW7ZsCdd+8IMfDOtDQ0NhPXqPq3s11MEvBQCAoykAABxNAQDgaAoAAEdTAAA4mgIAwNWOpCoqHhZRMapMtFPFEDPbKWcjjpmobiaamT22ej0ykVR1bHXNMluCZ17PzDUx069XJoao4q5zc3OVNRUbVcc+e/ZsZS0TKTXTUdyorl6PU6dOhfXo3LZt2xauzb5XGo1GZY1IKgCgrWgKAABHUwAAOJoCAMDRFAAAjqYAAHA0BQCAqz2nkN3+OpKZFVB1dWz1vErOQESyW2crmTy/klmfzXBHsjMSJWdW1BbVb7zxRmXt5MmTqWNHMnMIqq5mIJTe3t6wXvLzF9WfeuqpcK2aJbjppptaXp/97JrxSwEAcBGaAgDA0RQAAI6mAABwNAUAgKMpAAAcTQEA4GrPKWTyr2rf86wo75ydgYjyyJn5CbPyeeNWlZyByF4zpbu7u7KWya2r9Sp7Pjs7G9ZffvnlsD45OdnyYw8MDIT16H4KZ86cCdfOzMyE9WazWVlTr0f29Yqo76TongVmen4jMj09HdYz8zTcTwEA0FY0BQCAoykAABxNAQDgaAoAAEdTAAC4tkVSoyhgNmaY2aK6HRGtVo+diV+Wjm5GSl4zFQXMbF+tlIzaqvjkqVOnwvrp06fDehSRVFtIK1EkNYqUmpmdP38+rEeR7lWrVoVr16xZE9ZVLHRhYaGyprbtjtaa5baeHxkZCevRd6l67HZ8L/BLAQDgaAoAAEdTAAA4mgIAwNEUAACOpgAAcDQFAICrPaegstDr1q2rrJWeUyi5lWwm267WRvXS1yyTs87MEmReyzoyswSZa662kH799dfDujo3tf115OzZsy2vVY/b398f1sfGxiprO3fuDNdu2bIlrM/Pz4f1aI5BzVccPnw4rEdzJe9///vDtTfccENYV9t2R99pbJ0NAGgrmgIAwNEUAACOpgAAcDQFAICjKQAAHE0BAOBqzyn09PSE9SivXHLvf7M4265y79l7ImTWZmYF1LEzeeWS1yx7n4hO3mciujfA8ePHw7Wzs7NhXd1bIPLWW2+FdTVDMTg4WFkbHx8P1374wx8O65s3b66sqdknde+NzL051Ht8165dYT2agVDflX19fWFdfb6iOvdTAAC0FU0BAOBoCgAAR1MAADiaAgDA0RQAAI6mAABwtecUOpkPVzL3Uyg5S5BR8tjq+KXv5RDJPu+S1+2VV16prL388svhWpVNV/dTOHfuXGVN3RtA3RNh+/btlbVPfvKT4droPipm8edPzRmoz27J+5WoexpEMxaZ+Qmz3HcScwoAgLaiKQAAHE0BAOBoCgAAR1MAADiaAgDA1Y6kZpSMYNWpR1QUMLNNbbTVslkuPlkyepmNtalrGlHXTJ1b5rGnp6fD+j/+8Y/KmjpvFVNUj33mzJnK2uLiYrhWxSvXrFlTWRsaGgrXZrZ57qRMnNWsbAS/0/F/fikAABxNAQDgaAoAAEdTAAA4mgIAwNEUAACOpgAAcG2bU8jkw7MyuV21NsqAX6nbcivqtSyZoy557GazGdaPHj0a1hcWFiprPT094dqzZ8+G9bm5ubB+4cKFlh9bba39/PPPV9Z27NgRrh0ZGQnrpbd5jpT8TurkrQIi7ZgL4ZcCAMDRFAAAjqYAAHA0BQCAoykAABxNAQDgaAoAANe2OYUot5u9r0Amu17yXg2d3Pe89D0qSq3Nzl9k5kpeffXVcK2aJYjuSzAzMxOujeYMzPScQpQ/7+vrC9fOz8+H9ampqcra4cOHw7Xr168P6+o+EhE1Z6C+V6JrllmrqPdo9l4OUb0d3zn8UgAAOJoCAMDRFAAAjqYAAHA0BQCAoykAAFzbIqkldTJeGUXTsvGvzNa+JaO2WZ3cEvzUqVOVtddffz1cq84tiruqiKOKharIarQ99uzsbLhWvda9vb2VtRdeeCFcOzY2FtYnJiYqayr2qWK6J0+eDOunT5+urKnXY3BwMKxv2bKlsqa2E1cR4k7jlwIAwNEUAACOpgAAcDQFAICjKQAAHE0BAOBoCgAAV3tOQWW4M1vkZreazWxvrY5dcpYgymlntu6tIzq37GNHx85cTzOzZrMZ1l977bXKmsq9R3MIZvE8gNp2+9y5c2FdzTlEdZW5V69nNKegZiCeeOKJsP7cc8+F9cibb74Z1tXcSfR6qmsSzYWYma1Zs6ay9qlPfSpcu3v37rCu5hii79p2fG/wSwEA4GgKAABHUwAAOJoCAMDRFAAAjqYAAHA0BQCAqz1ckMn7d3Jvf0Xl5kvm+TPUfIU6t8xsR6aefa1V3j/K7KtrpuYYzp8/X+S8zPScgpoXiKj3+MzMTGVN3echWmtmduLEicqamgvJzD6Z5eaAFhYWwvr09HRl7cUXXwzX7tq1K6wPDQ2F9ZKfLzN+KQAALkJTAAA4mgIAwNEUAACOpgAAcDQFAIBr29bZJWNSKj4WnVtmrVnnopvqvLPPK0MdO/PYKqZ45syZsB5FKNWxVXQzipVGcdU6j62oyGpG9HqprcozW8+rtd3d3WE92r7azGx4eLiytm3btnDt2NhYWO/v76+srV27Nlx79dVXh/XMZ7sdn3t+KQAAHE0BAOBoCgAAR1MAADiaAgDA0RQAAI6mAABwtecUVHa2p6ensqaysyqDndkmWs0KlMz7l1xbctvuzOyGqqu1aototUV1lKtXswRqe+uorrbdVtQ20dH8RWZb+zr1iHofRrMC27dvD9d+4AMfCOvXXnttWI/mBaI5AzM9IxFdM/VaZr9zMrcpqINfCgAAR1MAADiaAgDA0RQAAI6mAABwNAUAgKMpAABc2+6nUHqP71aVzNwrmbXZbHk78sqtHjszp6DmEBYWFlquqzkENccQ3ctB3S9B5d6jOR8zs0ajUVnr7e0N16r9/Tds2NDyWlWfmJiorI2OjoZro+dspucBMt9J6thRPTtjlFmv3md18EsBAOBoCgAAR1MAADiaAgDA0RQAAI6mAABwNAUAgKs9p5DRyf3eS84hZOcvSu+LHsnMEihRjlrdO0PdT0HNA0TXdGlpKVyr5hiic1ezAgMDA2E9yvObme3atauyFt2zwMxs1apVYT26t4CaFVCZ+6ieWWuWu1dKyXseZJ9Xpq6eVx38UgAAOJoCAMDRFAAAjqYAAHA0BQCAoykAANy7EklVVLRTRQkz0VB17IyScdiSVCROyTyvCxcupB47ouKsqh7F/fr6+sK1IyMjYf3Tn/50WN+2bVtL52WWi26WjIVmo5lKFKfNPq9IduvszJbg7fhO4ZcCAMDRFAAAjqYAAHA0BQCAoykAABxNAQDgaAoAAFd7TkFteRwpOQtg1tktqCMl5xSyx85cs8xciJpDmJubC+vqvRTNGmTfh1HuXW1Pfd1114X1sbGxsN7d3V1Zy2zzrOrZzH3m2Jm8vqpnZyCi78PsXFXJ16sOfikAABxNAQDgaAoAAEdTAAA4mgIAwNEUAACOpgAAcLXnFNqRf2312Koe5eqz551Zn3leJecQzOIMePaxo/rCwkK4Vs0pqDmHaE5BPbYSZdvVnMKmTZvCejSHoB47+17I5PlLfi8omRkKdc0yswbZa9bp+6zwSwEA4GgKAABHUwAAOJoCAMDRFAAAjqYAAHC1I6kqJpWJf2UjdZkIV8nIaYaKxGVeD7W+5OvVbDbDtSpyqq55FDtV279nYoi9vb3h2tWrV4d1tU105vXKfH6y8cjosbNbY2fepyVj18s9cqrwSwEA4GgKAABHUwAAOJoCAMDRFAAAjqYAAHA0BQCA+5+YU4hk8/4lM9ytPq6Zfl4lH1vVo3mAs2fPhmvV9tZqjqFTcwqNRiNcq+pKyfdhZn0n34edPHa0PjMjlK2345rxSwEA4GgKAABHUwAAOJoCAMDRFAAAjqYAAHA0BQCAqz2noER55U7uH57Zp17p5H0gSs4xZGc7ovXnzp0L16pZgsysgVqrZPbQV9n1TurU/RTUNevkfQlKzmVlH7v09ym/FAAAjqYAAHA0BQCAoykAABxNAQDgaAoAAEdTAAC42nMK2UxxqbVqfcnMb/aaLNfZjuw1m5+fr6yp+yko6n4KzWazsqbmFNQsQVTPvhcye/CX/GyWvBdKyc+9WWdnCUqtfTeOzS8FAICjKQAAHE0BAOBoCgAAR1MAADiaAgDA1Y6knjlzJqxPTU1V1rIxqUxcLxupKxlZLRlJVddMbSkeUc8rip2++uqr4VoVOVWR1mhr7tnZ2XCtsri4WFmbnp4O177yyithPYrxmuWineq9kIl0Z95Hmc+1WWcjqdFjl35eUf3IkSPh2u3bt4d1M34pAAAuQlMAADiaAgDA0RQAAI6mAABwNAUAgKMpAABc11IUlgcAvKfwSwEA4GgKAABHUwAAOJoCAMDRFAAAjqYAAHA0BQCAoykAABxNAQDg/g+QOS9mANnv/gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get a batch of images and labels\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Take the first image in the batch\n",
    "image = images[0]\n",
    "\n",
    "# Reverse the normalization (optional, if you want to see the image in original scale)\n",
    "image = image / 2 + 0.5  # Unnormalize if you used Normalize((0.5,), (0.5,))\n",
    "\n",
    "# Convert tensor to numpy for visualization\n",
    "image = image.numpy().squeeze()  # Squeeze to remove extra dimension for grayscale\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(f'Label: {train_class_names[labels[0]]}')\n",
    "plt.axis('off')  # Hide axes for clarity\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4c5287",
   "metadata": {},
   "source": [
    "CAMERA STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ab0533",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''torch.save(model.state_dict(), 'asl_cnn_weights.pth')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d81feeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1q/snl996k91jzb97fx3w_zmck40000gn/T/ipykernel_36025/193514495.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('asl_cnn_weights.pth', map_location='cpu'))\n",
      "2025-04-10 15:54:34.646 python[36025:1439318] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam is open, ready to capture.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 15:54:38.089 python[36025:1439318] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-04-10 15:54:38.089 python[36025:1439318] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "# Load model and weights\n",
    "model = ASL_CNN()\n",
    "model.load_state_dict(torch.load('asl_cnn_weights.pth', map_location='cpu'))\n",
    "model.eval()\n",
    "\n",
    "# Define image transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Define class labels (you can load from train_dataset.classes if available)\n",
    "labels = [chr(i) for i in range(65, 91)]  # A-Z\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video stream.\")\n",
    "else:\n",
    "    print(\"Webcam is open, ready to capture.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image.\")\n",
    "        break\n",
    "\n",
    "    # Mirror the frame (horizontal flip)\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Increase the size of the square\n",
    "    h, w = frame.shape[:2]\n",
    "    box_size = 700  # Make the square 500px instead of 300px\n",
    "    top_left = (w // 2 - box_size // 2, h // 2 - box_size // 2)\n",
    "    bottom_right = (top_left[0] + box_size, top_left[1] + box_size)\n",
    "\n",
    "    # Draw rectangle\n",
    "    cv2.rectangle(frame, top_left, bottom_right, (0, 0, 255), 2)\n",
    "\n",
    "    # Get ROI\n",
    "    roi = frame[top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]]\n",
    "    if roi.shape[0] == 0 or roi.shape[1] == 0:\n",
    "        continue\n",
    "\n",
    "    # Preprocess ROI\n",
    "    try:\n",
    "        img_tensor = transform(roi).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor)\n",
    "            pred_idx = output.argmax().item()\n",
    "            confidence = torch.softmax(output, dim=1)[0, pred_idx].item() * 100\n",
    "            predicted_label = labels[pred_idx]\n",
    "\n",
    "        # Display prediction\n",
    "        msg = f\"{predicted_label}, Conf: {confidence:.1f}%\"\n",
    "        cv2.putText(frame, msg, (top_left[0], top_left[1] - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "\n",
    "    # Show the frame with the rectangle and prediction\n",
    "    cv2.imshow(\"ASL Live Prediction\", frame)\n",
    "\n",
    "    # Check if 'q' is pressed to break loop\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):  # If 'q' is pressed, exit the loop\n",
    "        print(\"Exiting...\")\n",
    "        break\n",
    "\n",
    "# Cleanup: release the camera and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
