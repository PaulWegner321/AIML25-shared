{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ibm_watsonx_ai import Credentials, APIClient\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.foundation_models.schema import TextGenParameters\n",
    "from decouple import Config, RepositoryEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables using python-decouple\n",
    "# The .env file should be in the root of the project\n",
    "# The .env file should NOT be committed to the repository\n",
    "\n",
    "config = Config(RepositoryEnv(\".env.paul\"))\n",
    "\n",
    "# Load the credentials\n",
    "WX_API_KEY = config(\"WX_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = Credentials(\n",
    "    url = \"https://us-south.ml.cloud.ibm.com\",\n",
    "    api_key = WX_API_KEY\n",
    ")\n",
    "client = APIClient(\n",
    "    credentials=credentials, \n",
    "    project_id=\"68126b74-155e-4a70-aa2c-1781dfad87f6\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_id': 'ibm/granite-3-8b-instruct',\n",
       " 'model_version': '1.1.0',\n",
       " 'created_at': '2025-04-29T11:42:50.710Z',\n",
       " 'results': [{'generated_text': \"\\n\\nI'm currently in Copenhagen, Denmark, for a conference. It's a beautiful city with a rich history and a vibrant cultural scene. I've been exploring the city, visiting iconic landmarks, and enjoying the local cuisine.\\n\\nOne of the highlights of my trip so far has been visiting the Little Mermaid statue, a beloved symbol of Copenhagen. The statue, based on the fairy tale by Hans Christian Andersen, is a must-see for anyone visiting the city.\\n\\nI've also been indulging in Danish pastries, such as the famous kanelsnegl (cinnamon swirls) and smÃ¸rrebrÃ¸d (open-faced sandwiches). The local coffee culture is also something I've come to appreciate, with cozy cafes serving delicious coffee and pastries.\\n\\nIn addition to sightseeing, I've been attending conference sessions and networking with fellow professionals. It's been an enlightening experience, with many insightful discussions and new ideas to bring back home.\\n\\nI'm looking forward to the rest of my stay in Copenhagen, as I continue to explore the city and immerse myself in its unique charm. If you have any recommendations for places to visit or things to do in Copenhagen, I'd love to hear them!\\n\\nStay tuned for more updates from my adventure in Denmark.\\n\\nBest regards,\\n[Your Name]\",\n",
       "   'generated_token_count': 342,\n",
       "   'input_token_count': 7,\n",
       "   'stop_reason': 'eos_token'}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Setup the Model (Granite-13b-chat-v2) ---\n",
    "params = TextGenParameters(\n",
    "    temperature=0.2,            # Light creativity\n",
    "    max_new_tokens=3000          # Enough for descriptive text\n",
    ")\n",
    "\n",
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    params=params,\n",
    "    model_id=\"ibm/granite-3-8b-instruct\",\n",
    ")\n",
    "prompt = \"Hello from copenhagen!\"\n",
    "generated_response = model.generate(prompt)\n",
    "\n",
    "generated_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(sign_name: str) -> str:\n",
    "    return (\n",
    "        f\"You are an American Sign Language (ASL) teacher.\\n\\n\"\n",
    "        f\"Please clearly explain how to perform the ASL sign on a beginner level for '{sign_name}'. \"\n",
    "        f\"Use simple language and full sentences. Do not assume any prior knowledge about ASL.\\n\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate ASL Description ---\n",
    "def generate_asl_description(sign_name: str) -> str:\n",
    "    prompt = create_prompt(sign_name)\n",
    "    \n",
    "    response = model.generate(prompt=prompt)\n",
    "    generated_text = response['results'][0]['generated_text']\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Generated ASL Description:\n",
      "\n",
      "\n",
      "To sign 'laptop' in American Sign Language (ASL), follow these steps:\n",
      "\n",
      "1. Start with your dominant hand open and facing up, palm facing you.\n",
      "2. Bring your hand down to your lap, as if you're placing an object there.\n",
      "3. With your hand still in your lap, bend your index finger and middle finger down to touch the tips of your thumb, forming a 'C' shape.\n",
      "4. Keep your ring finger and pinky finger straight and slightly apart.\n",
      "5. Maintain this 'C' shape with your fingers while keeping your hand in your lap.\n",
      "6. To indicate that it's a laptop, you can add a small, quick, circular motion with your hand, as if you're turning on a device.\n",
      "\n",
      "Remember, practice makes perfect. Keep practicing this sign until you feel comfortable with it.\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    sign_to_explain = input(\"ðŸ“ Enter the ASL sign you want explained: \")\n",
    "\n",
    "    try:\n",
    "        description = generate_asl_description(sign_to_explain)\n",
    "        print(\"\\nâœ… Generated ASL Description:\\n\")\n",
    "        print(description)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during generation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_judge_prompt(sign_name: str, description: str) -> str:\n",
    "    return f\"\"\"You are evaluating the quality of a generated ASL (American Sign Language) sign description based on 3 criteria:\n",
    "\n",
    "1. Faithfulness: Is the description factually consistent with known ASL practices (no hallucinations)?\n",
    "2. Answer Relevance: Does the description directly and completely describe the sign requested?\n",
    "3. Context Relevance: Would a typical ASL reference source support this description for the given sign?\n",
    "\n",
    "Use the following scale:\n",
    "- 1 = fully meets the criterion\n",
    "- 0.5 = partially meets the criterion\n",
    "- 0 = does not meet the criterion\n",
    "\n",
    "ASL Sign Request:\n",
    "\"{sign_name}\"\n",
    "\n",
    "Generated Description:\n",
    "\"{description}\"\n",
    "\n",
    "Respond as a JSON object, exactly like this (no extra text or comments):\n",
    "\n",
    "{{\n",
    "  \"faithfulness\": <0, 0.5, or 1>,\n",
    "  \"answer_relevance\": <0, 0.5, or 1>,\n",
    "  \"context_relevance\": <0, 0.5, or 1>\n",
    "}}\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_description(sign_name: str, description: str) -> dict:\n",
    "    import json, re\n",
    "\n",
    "    judge_prompt = create_judge_prompt(sign_name, description)\n",
    "    response = model.generate(prompt=judge_prompt)\n",
    "    judged_text = response['results'][0]['generated_text']\n",
    "\n",
    "    # Default fallback result\n",
    "    judge_result = {\n",
    "        \"faithfulness\": None,\n",
    "        \"answer_relevance\": None,\n",
    "        \"context_relevance\": None\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(judged_text)\n",
    "        if isinstance(parsed, dict):\n",
    "            judge_result.update(parsed)\n",
    "    except Exception:\n",
    "        matches = re.findall(r\"\\{.*?\\}\", judged_text, re.DOTALL)\n",
    "        if matches:\n",
    "            try:\n",
    "                parsed = json.loads(matches[0])\n",
    "                if isinstance(parsed, dict):\n",
    "                    judge_result.update(parsed)\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Fallback parse failed: {e}\")\n",
    "        else:\n",
    "            print(\"âŒ No JSON object found in model output.\")\n",
    "\n",
    "    return judge_result\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‹ Welcome to the ASL Sign Generator using Granite-13b!\n",
      "\n",
      "âœ… Generated ASL Description:\n",
      "\n",
      "\n",
      "To sign 'car' in American Sign Language (ASL), follow these steps:\n",
      "\n",
      "1. Start with your dominant hand open and facing up, representing the general concept of a vehicle.\n",
      "2. Bring your hand down to your waist level, moving it in a small circular motion to indicate the circular shape of a car's body.\n",
      "3. Extend your index finger and middle finger, keeping them together, to represent the car's windshield.\n",
      "4. Bend your index finger slightly to show the curve of the windshield.\n",
      "5. Move your hand forward and slightly to the right, as if you're pointing to a car parked in front of you.\n",
      "\n",
      "Remember, ASL is a visual language, so practice the motion and rhythm to ensure you're signing 'car' accurately. Keep practicing, and you'll become more comfortable with the sign.\n",
      "\n",
      "ðŸ“Š Evaluation Result:\n",
      " Faithfulness: 1\n",
      " Answer Relevance: 1\n",
      " Context Relevance: 1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸ‘‹ Welcome to the ASL Sign Generator using Granite-13b!\")\n",
    "    sign_to_explain = input(\"ðŸ“ Enter the ASL sign you want explained: \")\n",
    "\n",
    "    try:\n",
    "        # Generate ASL description\n",
    "        description = generate_asl_description(sign_to_explain)\n",
    "        print(\"\\nâœ… Generated ASL Description:\\n\")\n",
    "        print(description)\n",
    "\n",
    "        # Judge the description\n",
    "        judge_result = evaluate_description(sign_to_explain, description)\n",
    "\n",
    "        print(\"\\nðŸ“Š Evaluation Result:\")\n",
    "        print(f\" Faithfulness: {judge_result.get('faithfulness')}\")\n",
    "        print(f\" Answer Relevance: {judge_result.get('answer_relevance')}\")\n",
    "        print(f\" Context Relevance: {judge_result.get('context_relevance')}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during generation or evaluation: {e}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mass evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_batch(signs: list[str], reference_definitions: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate multiple ASL signs using the generate + judge pipeline.\n",
    "    Returns a DataFrame with individual scores and definitions.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for sign, expected in zip(signs, reference_definitions):\n",
    "        try:\n",
    "            # Step 1: Generate ASL description\n",
    "            generated = generate_asl_description(sign)\n",
    "\n",
    "            # Step 2: Evaluate the description\n",
    "            scores = evaluate_description(sign, generated)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error for sign '{sign}': {e}\")\n",
    "            generated = \"ERROR\"\n",
    "            scores = {\n",
    "                \"faithfulness\": None,\n",
    "                \"answer_relevance\": None,\n",
    "                \"context_relevance\": None\n",
    "            }\n",
    "\n",
    "        results.append({\n",
    "            \"Sign\": sign,\n",
    "            \"Expected Definition\": expected,\n",
    "            \"Generated Description\": generated,\n",
    "            \"Faithfulness\": scores.get(\"faithfulness\"),\n",
    "            \"Answer Relevance\": scores.get(\"answer_relevance\"),\n",
    "            \"Context Relevance\": scores.get(\"context_relevance\")\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # Print the full table\n",
    "    print(\"\\nðŸ“‹ Evaluation Table:\\n\")\n",
    "    print(df)\n",
    "\n",
    "    # Print average scores\n",
    "    avg_faithfulness = df[\"Faithfulness\"].dropna().mean()\n",
    "    avg_relevance = df[\"Answer Relevance\"].dropna().mean()\n",
    "    avg_context = df[\"Context Relevance\"].dropna().mean()\n",
    "\n",
    "    print(\"\\nðŸ“Š Average Scores:\")\n",
    "    print(f\"Faithfulness: {avg_faithfulness:.2f}\")\n",
    "    print(f\"Answer Relevance: {avg_relevance:.2f}\")\n",
    "    print(f\"Context Relevance: {avg_context:.2f}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "signs = [\"salad\", \"pizza\"]\n",
    "definitions = [\n",
    "    \"Mimic the motion of tossing a salad by placing both hands next to each other in front of your with your palms oriented toward each other, and then slightly moving your hands toward each other and up a couple of times.\",\n",
    "    \"Place your hand in front of your face with your fingers pointing at your mouth. Then, slightly move your hand toward your mouth a couple of times.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ Evaluation Table:\n",
      "\n",
      "    Sign                                Expected Definition  \\\n",
      "0  salad  Mimic the motion of tossing a salad by placing...   \n",
      "1  pizza  Place your hand in front of your face with you...   \n",
      "\n",
      "                               Generated Description  Faithfulness  \\\n",
      "0  \\nTo sign 'salad' in American Sign Language (A...             1   \n",
      "1  \\nTo sign 'pizza' in American Sign Language (A...             1   \n",
      "\n",
      "   Answer Relevance  Context Relevance  \n",
      "0                 1                  1  \n",
      "1                 1                  1  \n",
      "\n",
      "ðŸ“Š Average Scores:\n",
      "Faithfulness: 1.00\n",
      "Answer Relevance: 1.00\n",
      "Context Relevance: 1.00\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_judge_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      5\u001b[39m scores = {\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfaithfulness\u001b[39m\u001b[33m\"\u001b[39m: df[\u001b[33m\"\u001b[39m\u001b[33mFaithfulness\u001b[39m\u001b[33m\"\u001b[39m].dropna().mean(),\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33manswer_relevance\u001b[39m\u001b[33m\"\u001b[39m: df[\u001b[33m\"\u001b[39m\u001b[33mAnswer Relevance\u001b[39m\u001b[33m\"\u001b[39m].dropna().mean(),\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcontext_relevance\u001b[39m\u001b[33m\"\u001b[39m: df[\u001b[33m\"\u001b[39m\u001b[33mContext Relevance\u001b[39m\u001b[33m\"\u001b[39m].dropna().mean()\n\u001b[32m      9\u001b[39m }\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Step 3: Plot the result\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mplot_judge_scores\u001b[49m(scores)\n",
      "\u001b[31mNameError\u001b[39m: name 'plot_judge_scores' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 1: Evaluate all signs\n",
    "df = evaluate_batch(signs, definitions)\n",
    "\n",
    "# Step 2: Calculate average scores\n",
    "scores = {\n",
    "    \"faithfulness\": df[\"Faithfulness\"].dropna().mean(),\n",
    "    \"answer_relevance\": df[\"Answer Relevance\"].dropna().mean(),\n",
    "    \"context_relevance\": df[\"Context Relevance\"].dropna().mean()\n",
    "}\n",
    "\n",
    "# Step 3: Plot the result\n",
    "plot_judge_scores(scores)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
