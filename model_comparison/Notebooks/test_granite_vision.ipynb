{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Granite Vision ASL Recognition\n",
    "\n",
    "This notebook demonstrates how to use the IBM Granite Vision model for ASL letter recognition. The code includes:\n",
    "\n",
    "- Setting up the WatsonX API client\n",
    "- Processing images for analysis\n",
    "- Implementing different prompting strategies\n",
    "- Handling model responses and extracting predictions\n",
    "\n",
    "### Note: This notebook requires WatsonX API credentials in the backend/.env file\n",
    "### Please refer to test_granite_vision.py for the actually implemented script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import base64\n",
    "import time\n",
    "import requests\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import io\n",
    "from dotenv import load_dotenv\n",
    "from typing import Literal\n",
    "from io import BytesIO\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environment Variables and Initialize Client\n",
    "\n",
    "Load API credentials from .env file for WatsonX API access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .env file from: /Users/henrikjacobsen/Desktop/CBS/Semester 2/Artifical Intelligence and Machine Learning/Final Project/AIML25-shared/backend/.env\n",
      "API key found: Yes\n",
      "Project ID found: Yes\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from backend/.env\n",
    "# Use an absolute path to the .env file\n",
    "dotenv_path = \"/Users/henrikjacobsen/Desktop/CBS/Semester 2/Artifical Intelligence and Machine Learning/Final Project/AIML25-shared/backend/.env\"\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "print(f\"Loading .env file from: {dotenv_path}\")\n",
    "\n",
    "# Get WatsonX credentials\n",
    "WATSONX_API_KEY = os.getenv(\"WATSONX_API_KEY\")\n",
    "WATSONX_PROJECT_ID = os.getenv(\"WATSONX_PROJECT_ID\")\n",
    "GRANITE_MODEL_ID = \"ibm/granite-vision-3-2-2b\"\n",
    "\n",
    "print(f\"API key found: {'Yes' if WATSONX_API_KEY else 'No'}\")\n",
    "print(f\"Project ID found: {'Yes' if WATSONX_PROJECT_ID else 'No'}\")\n",
    "\n",
    "if not WATSONX_API_KEY or not WATSONX_PROJECT_ID:\n",
    "    raise ValueError(\"WatsonX API key or Project ID not found in environment variables. Please check your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "Define prompt templates for different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "PROMPT_TEMPLATES = {\n",
    "\"zero_shot\": \"\"\"You are an expert in American Sign Language (ASL) recognition. Analyze the provided image and identify the ASL letter being signed (A-Z).\n",
    "\n",
    "Respond only with a valid JSON object, using this format:\n",
    "{\n",
    "  \"letter\": \"A single uppercase letter (A-Z)\",\n",
    "  \"confidence\": \"confidence score (0-1)\",\n",
    "  \"feedback\": \"A short explanation of how the gesture maps to the predicted letter\"\n",
    "}\n",
    "Be precise and avoid adding anything outside the JSON response.\"\"\",\n",
    "\n",
    "\"few_shot\": \"\"\"You are an expert in American Sign Language (ASL) recognition. Analyze the provided image and identify the ASL letter being signed (A-Z).\n",
    "\n",
    "Here are some known ASL hand signs:\n",
    "- A: Fist with thumb resting on the side\n",
    "- B: Flat open hand, fingers extended upward, thumb across the palm\n",
    "- C: Hand curved into the shape of the letter C\n",
    "- D: Index finger up, thumb touching middle finger forming an oval\n",
    "- E: Fingers bent, thumb tucked under\n",
    "\n",
    "Respond only with a JSON object like this:\n",
    "{\n",
    "  \"letter\": \"A single uppercase letter (A-Z)\",\n",
    "  \"confidence\": \"confidence score (0-1)\",\n",
    "  \"feedback\": \"Why this gesture matches the predicted letter\"\n",
    "}\n",
    "Only return the JSON object. No explanations before or after.\"\"\",\n",
    "\n",
    "\"chain_of_thought\": \"\"\"You are an expert in American Sign Language (ASL) recognition. Carefully analyze the provided image step-by-step to identify the ASL letter (A-Z).\n",
    "\n",
    "1. Describe the hand shape\n",
    "2. Describe the finger and thumb positions\n",
    "3. Compare these to known ASL letter signs\n",
    "4. Identify the most likely letter\n",
    "\n",
    "Then output your answer as JSON:\n",
    "{\n",
    "  \"letter\": \"A single uppercase letter (A-Z)\",\n",
    "  \"confidence\": \"confidence score (0-1),\n",
    "  \"feedback\": \"Summarize your reasoning in one sentence\"\n",
    "}\n",
    "Return only the JSON object with no extra text.\"\"\",\n",
    "\n",
    "\"visual_grounding\": \"\"\"You are an expert in American Sign Language (ASL) recognition. Carefully analyze the provided image of a hand gesture and determine which ASL letter (A–Z) it represents.\n",
    "\n",
    "To guide your analysis, consider the following:\n",
    "- Which fingers are extended or bent?\n",
    "- Is the thumb visible, and where is it positioned?\n",
    "- What is the orientation of the palm (facing forward, sideways, etc.)?\n",
    "- Are there any unique shapes formed (e.g., circles, fists, curves)?\n",
    "\n",
    "Now, based on this visual inspection, provide your prediction in the following JSON format:\n",
    "\n",
    "{\n",
    "  \"letter\": \"predicted letter (A-Z)\",\n",
    "  \"confidence\": \"confidence score (0–1)\",\n",
    "  \"feedback\": \"brief explanation describing the observed hand shape and reasoning\"\n",
    "}\n",
    "\n",
    "Be precise, use visual clues from the image, and avoid guessing without justification.\"\"\",\n",
    "\n",
    "\"contrastive\": \"\"\"You are an expert in American Sign Language (ASL) recognition. Analyze the provided image of a hand gesture and identify the correct ASL letter.\n",
    "\n",
    "Consider the following candidate letters: A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z\n",
    "(These letters are visually similar and often confused.)\n",
    "\n",
    "Step-by-step:\n",
    "1. Observe the hand shape, finger positions, and thumb placement.\n",
    "2. Compare the observed gesture against the typical signs for each candidate letter.\n",
    "3. Eliminate unlikely candidates based on visible differences.\n",
    "4. Choose the most plausible letter and explain your reasoning.\n",
    "\n",
    "Format your response as JSON:\n",
    "\n",
    "{\n",
    "  \"letter\": \"predicted letter from candidates\",\n",
    "  \"confidence\": \"confidence score (0–1)\",\n",
    "  \"feedback\": \"why this letter was selected over the others\"\n",
    "}\n",
    "\n",
    "Be analytical and compare carefully to avoid misclassification.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API and Authentication Functions\n",
    "\n",
    "Define functions to handle WatsonX API authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_watsonx_token(api_key):\n",
    "    \"\"\"Get or refresh the IBM Cloud IAM token.\"\"\"\n",
    "    auth_url = \"https://iam.cloud.ibm.com/identity/token\"\n",
    "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "    data = {\"grant_type\": \"urn:ibm:params:oauth:grant-type:apikey\", \"apikey\": api_key}\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(auth_url, headers=headers, data=data, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        token_data = response.json()\n",
    "        return token_data.get(\"access_token\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error getting WatsonX token: {e}\")\n",
    "        return None\n",
    "\n",
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"Estimate the number of tokens in a text using a simple approximation.\"\"\"\n",
    "    # Rough estimation: 1 token ≈ 4 characters for English text\n",
    "    # This is a conservative estimate that tends to be higher than actual token count\n",
    "    return max(1, len(text) // 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing Functions\n",
    "\n",
    "Define functions to process images for the WatsonX API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_base64(image_path):\n",
    "    \"\"\"Encode image to base64 string with proper format for Granite Vision.\"\"\"\n",
    "    try:\n",
    "        # Open and convert to RGB\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Calculate new dimensions while maintaining aspect ratio\n",
    "        max_size = 1024\n",
    "        ratio = min(max_size / img.width, max_size / img.height)\n",
    "        new_size = (int(img.width * ratio), int(img.height * ratio))\n",
    "        \n",
    "        # Resize with high-quality resampling\n",
    "        img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "            \n",
    "        # Save to bytes with JPEG format and maximum quality\n",
    "        buffer = BytesIO()\n",
    "        img.save(buffer, format='JPEG', quality=100, optimize=True)\n",
    "        buffer.seek(0)\n",
    "            \n",
    "        # Encode to base64\n",
    "        img_str = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "        \n",
    "        # Clean the base64 string\n",
    "        img_str = img_str.replace('\\n', '')\n",
    "        \n",
    "        # Create the full data URI with proper MIME type\n",
    "        data_uri = f\"data:image/jpeg;base64,{img_str}\"\n",
    "            \n",
    "        # Log the data URI format (truncated for readability)\n",
    "        logging.info(f\"Image data URI format: {data_uri[:100]}...\")\n",
    "        \n",
    "        return data_uri\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error encoding image: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASL Prediction Function\n",
    "\n",
    "Define the main function for getting ASL predictions from the Granite Vision model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_asl_prediction(image_path: str, prompt_strategy: Literal[\"zero_shot\", \"few_shot\", \"chain_of_thought\", \"visual_grounding\", \"contrastive\"] = \"zero_shot\") -> dict:\n",
    "    \"\"\"Get ASL prediction from Granite Vision model.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        token = get_watsonx_token(WATSONX_API_KEY)\n",
    "        if not token:\n",
    "            return {\n",
    "                \"error\": \"Failed to get authentication token\",\n",
    "                \"metadata\": {\n",
    "                    \"response_time\": round(time.time() - start_time, 3),\n",
    "                    \"model\": \"granite_vision\",\n",
    "                    \"strategy\": prompt_strategy\n",
    "                }\n",
    "            }\n",
    "\n",
    "        # Process image and get data URI\n",
    "        image_data_uri = encode_image_base64(image_path)\n",
    "\n",
    "        # Get the appropriate prompt template\n",
    "        prompt_template = PROMPT_TEMPLATES.get(prompt_strategy, PROMPT_TEMPLATES[\"zero_shot\"])\n",
    "\n",
    "        # Create the payload with the actual ASL prediction request\n",
    "        payload = {\n",
    "            \"model_id\": GRANITE_MODEL_ID,\n",
    "            \"project_id\": WATSONX_PROJECT_ID,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prompt_template\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": image_data_uri\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"temperature\": 0.05,\n",
    "            \"top_p\": 1.0,\n",
    "            \"max_tokens\": 300\n",
    "        }\n",
    "\n",
    "        # Make the API request\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        response = requests.post(\n",
    "            \"https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2023-05-29\",\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "            timeout=60\n",
    "        )\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        \n",
    "        # Extract the generated text\n",
    "        generated_text = result.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "        \n",
    "        # Clean the response text - remove any markdown code blocks\n",
    "        generated_text = generated_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        \n",
    "        # Parse the JSON response\n",
    "        try:\n",
    "            prediction = json.loads(generated_text)\n",
    "            if not isinstance(prediction, dict):\n",
    "                raise ValueError(\"Response is not a valid JSON object\")\n",
    "            if \"letter\" not in prediction:\n",
    "                raise ValueError(\"Response missing 'letter' field\")\n",
    "\n",
    "            # Calculate response time\n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            # Return the prediction in the format expected by evaluate_models.py\n",
    "            return {\n",
    "                \"letter\": prediction[\"letter\"],\n",
    "                \"confidence\": prediction.get(\"confidence\", 0.0),\n",
    "                \"feedback\": prediction.get(\"feedback\", \"\"),\n",
    "                \"metadata\": {\n",
    "                    \"response_time\": round(response_time, 3),\n",
    "                    \"model\": \"granite_vision\",\n",
    "                    \"strategy\": prompt_strategy\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.error(f\"Failed to parse JSON response: {e}\")\n",
    "            logging.error(f\"Raw response: {generated_text}\")\n",
    "            return {\n",
    "                \"error\": \"Invalid JSON response from model\",\n",
    "                \"raw_response\": generated_text,\n",
    "                \"metadata\": {\n",
    "                    \"response_time\": round(time.time() - start_time, 3),\n",
    "                    \"model\": \"granite_vision\",\n",
    "                    \"strategy\": prompt_strategy\n",
    "                }\n",
    "            }\n",
    "        except ValueError as e:\n",
    "            logging.error(f\"Invalid response format: {e}\")\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"metadata\": {\n",
    "                    \"response_time\": round(time.time() - start_time, 3),\n",
    "                    \"model\": \"granite_vision\",\n",
    "                    \"strategy\": prompt_strategy\n",
    "                }\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        response_time = time.time() - start_time\n",
    "        logging.error(f\"Error in ASL prediction: {e}\")\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"metadata\": {\n",
    "                \"response_time\": round(response_time, 3),\n",
    "                \"model\": \"granite_vision\",\n",
    "                \"strategy\": prompt_strategy\n",
    "            }\n",
    "        }\n",
    "    finally:\n",
    "        # Add a delay to respect rate limits\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Sample Image\n",
    "\n",
    "Test the model with a sample image using different prompting strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this path to your image\n",
    "base_path = Path(\"/Users/henrikjacobsen/Desktop/CBS/Semester 2/Artifical Intelligence and Machine Learning/Final Project/AIML25-shared/model_comparison\")\n",
    "image_path = base_path / \"data/V/V_0_20250428_114109.jpg\"\n",
    "\n",
    "# Make sure the path exists\n",
    "if not image_path.exists():\n",
    "    print(f\"Image not found: {image_path}\")\n",
    "    # Try to find any image in the dataset\n",
    "    data_dir = base_path / \"data\"\n",
    "    if data_dir.exists():\n",
    "        for letter_dir in data_dir.glob(\"*\"):\n",
    "            if letter_dir.is_dir():\n",
    "                for img_file in letter_dir.glob(\"*.jpg\"):\n",
    "                    image_path = img_file\n",
    "                    print(f\"Using alternative image: {image_path}\")\n",
    "                    break\n",
    "            if image_path.exists():\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a specific strategy\n",
    "def test_single_strategy(strategy=\"zero_shot\"):\n",
    "    \"\"\"Test the model with a single prompting strategy.\"\"\"\n",
    "    print(f\"\\nTesting with {strategy} strategy...\")\n",
    "    print(f\"Using Granite Vision on image: {image_path}\")\n",
    "    \n",
    "    result = get_asl_prediction(str(image_path), strategy)\n",
    "    print(f\"\\nResult:\\n{json.dumps(result, indent=2)}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with zero_shot strategy...\n",
      "Using Granite Vision on image: /Users/henrikjacobsen/Desktop/CBS/Semester 2/Artifical Intelligence and Machine Learning/Final Project/AIML25-shared/model_comparison/data/V/V_0_20250428_114109.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 12:02:41,035 - INFO - Image data URI format: data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBA...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result:\n",
      "{\n",
      "  \"letter\": \"V\",\n",
      "  \"confidence\": 0.95,\n",
      "  \"feedback\": \"The gesture is a classic V sign, which is used to represent the letter 'V' in ASL.\",\n",
      "  \"metadata\": {\n",
      "    \"response_time\": 2.375,\n",
      "    \"model\": \"granite_vision\",\n",
      "    \"strategy\": \"zero_shot\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Run the test with the zero_shot strategy\n",
    "result = test_single_strategy(\"zero_shot\")\n",
    "\n",
    "# Uncomment to test all strategies\n",
    "# def test_all_strategies():\n",
    "#     \"\"\"Test the model with all prompting strategies.\"\"\"\n",
    "#     results = {}\n",
    "#     \n",
    "#     for strategy in PROMPT_TEMPLATES.keys():\n",
    "#         print(f\"\\nTesting with {strategy} strategy...\")\n",
    "#         result = get_asl_prediction(str(image_path), strategy)\n",
    "#         results[strategy] = result\n",
    "#         \n",
    "#         print(f\"Result:\\n{json.dumps(result, indent=2)}\")\n",
    "#         \n",
    "#     return results\n",
    "# \n",
    "# all_results = test_all_strategies()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
