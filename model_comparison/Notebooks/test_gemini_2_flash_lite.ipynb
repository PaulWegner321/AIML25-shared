{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini 2.0 Flash-Lite ASL Recognition\n",
    "\n",
    "This notebook demonstrates how to use the Gemini 2.0 Flash-Lite model for ASL letter recognition. The code includes:\n",
    "\n",
    "- Setting up the Gemini API client\n",
    "- Processing images for analysis\n",
    "- Implementing different prompting strategies\n",
    "- Handling model responses and extracting predictions\n",
    "\n",
    "### Note: This notebook requires Google API credentials in the backend/.env file\n",
    "### Please refer to test_gemini_2_flash_lite.py for the actually implemented script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import base64\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from PIL import Image\n",
    "import io\n",
    "import time\n",
    "from typing import Literal\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environment Variables and Initialize Client\n",
    "\n",
    "Load API credentials from .env file and initialize the Gemini client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .env file from: /Users/henrikjacobsen/Desktop/CBS/Semester 2/Artifical Intelligence and Machine Learning/Final Project/AIML25-shared/backend/.env\n",
      "API key found: Yes\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from backend/.env\n",
    "# Use an absolute path to the .env file\n",
    "dotenv_path = \"/Users/henrikjacobsen/Desktop/CBS/Semester 2/Artifical Intelligence and Machine Learning/Final Project/AIML25-shared/backend/.env\"\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "print(f\"Loading .env file from: {dotenv_path}\")\n",
    "\n",
    "# Get Google API credentials\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "print(f\"API key found: {'Yes' if GOOGLE_API_KEY else 'No'}\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"Google API key not found in environment variables. Please check your .env file.\")\n",
    "\n",
    "# Initialize the Gemini client\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration and Prompt Templates\n",
    "\n",
    "Define model configuration and prompt templates for different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    \"flash-lite\": {\n",
    "        \"name\": \"gemini-2.0-flash-lite\",\n",
    "        \"display_name\": \"Gemini 2.0 Flash-Lite\",\n",
    "        \"rate_limit_delay\": 3,  # 3 seconds between requests (faster than Flash)\n",
    "        \"max_retries\": 3\n",
    "    }\n",
    "}\n",
    "\n",
    "# Prompt templates\n",
    "PROMPT_TEMPLATES = {\n",
    "\"zero_shot\": \"\"\"You are an expert in American Sign Language (ASL) recognition. Analyze the provided image and identify the ASL letter being signed (A-Z).\n",
    "\n",
    "Respond only with a valid JSON object, using this format:\n",
    "{\n",
    "  \"letter\": \"A single uppercase letter (A-Z)\",\n",
    "  \"confidence\": \"confidence score (0-1)\",\n",
    "  \"feedback\": \"A short explanation of how the gesture maps to the predicted letter\"\n",
    "}\n",
    "Be precise and avoid adding anything outside the JSON response.\"\"\",\n",
    "\n",
    "\"few_shot\": \"\"\"You are an expert in American Sign Language (ASL) recognition. Analyze the provided image and identify the ASL letter being signed (A-Z).\n",
    "\n",
    "Here are some known ASL hand signs:\n",
    "- A: Fist with thumb resting on the side\n",
    "- B: Flat open hand, fingers extended upward, thumb across the palm\n",
    "- C: Hand curved into the shape of the letter C\n",
    "- D: Index finger up, thumb touching middle finger forming an oval\n",
    "- E: Fingers bent, thumb tucked under\n",
    "\n",
    "Respond only with a JSON object like this:\n",
    "{\n",
    "  \"letter\": \"A single uppercase letter (A-Z)\",\n",
    "  \"confidence\": \"confidence score (0-1)\",\n",
    "  \"feedback\": \"Why this gesture matches the predicted letter\"\n",
    "}\n",
    "Only return the JSON object. No explanations before or after.\"\"\",\n",
    "\n",
    "\"chain_of_thought\": \"\"\"You are an expert in American Sign Language (ASL) recognition. Carefully analyze the provided image step-by-step to identify the ASL letter (A-Z).\n",
    "\n",
    "1. Describe the hand shape\n",
    "2. Describe the finger and thumb positions\n",
    "3. Compare these to known ASL letter signs\n",
    "4. Identify the most likely letter\n",
    "\n",
    "Then output your answer as JSON:\n",
    "{\n",
    "  \"letter\": \"A single uppercase letter (A-Z)\",\n",
    "  \"confidence\": \"confidence score (0-1),\n",
    "  \"feedback\": \"Summarize your reasoning in one sentence\"\n",
    "}\n",
    "Return only the JSON object with no extra text.\"\"\",\n",
    "\n",
    "\"visual_grounding\": \"\"\"You are an expert in American Sign Language (ASL) recognition. Carefully analyze the provided image of a hand gesture and determine which ASL letter (A–Z) it represents.\n",
    "\n",
    "To guide your analysis, consider the following:\n",
    "- Which fingers are extended or bent?\n",
    "- Is the thumb visible, and where is it positioned?\n",
    "- What is the orientation of the palm (facing forward, sideways, etc.)?\n",
    "- Are there any unique shapes formed (e.g., circles, fists, curves)?\n",
    "\n",
    "Now, based on this visual inspection, provide your prediction in the following JSON format:\n",
    "\n",
    "{\n",
    "  \"letter\": \"predicted letter (A-Z)\",\n",
    "  \"confidence\": \"confidence score (0–1)\",\n",
    "  \"feedback\": \"brief explanation describing the observed hand shape and reasoning\"\n",
    "}\n",
    "\n",
    "Be precise, use visual clues from the image, and avoid guessing without justification.\"\"\",\n",
    "\n",
    "\"contrastive\": \"\"\"You are an expert in American Sign Language (ASL) recognition. Analyze the provided image of a hand gesture and identify the correct ASL letter.\n",
    "\n",
    "Consider the following candidate letters: A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z\n",
    "(These letters are visually similar and often confused.)\n",
    "\n",
    "Step-by-step:\n",
    "1. Observe the hand shape, finger positions, and thumb placement.\n",
    "2. Compare the observed gesture against the typical signs for each candidate letter.\n",
    "3. Eliminate unlikely candidates based on visible differences.\n",
    "4. Choose the most plausible letter and explain your reasoning.\n",
    "\n",
    "Format your response as JSON:\n",
    "\n",
    "{\n",
    "  \"letter\": \"predicted letter from candidates\",\n",
    "  \"confidence\": \"confidence score (0–1)\",\n",
    "  \"feedback\": \"why this letter was selected over the others\"\n",
    "}\n",
    "\n",
    "Be analytical and compare carefully to avoid misclassification.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing Functions\n",
    "\n",
    "Define functions to process images for the Gemini API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_convert_image(image_path, target_format=\"JPEG\", quality=95, max_size=(512, 512)):\n",
    "    \"\"\"Process image and return raw bytes and MIME type.\"\"\"\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            # Log original image format\n",
    "            logging.debug(f\"Original image format: {img.format}\")\n",
    "            \n",
    "            # Convert to RGB if necessary\n",
    "            if img.mode != 'RGB':\n",
    "                logging.debug(f\"Converting image from {img.mode} to RGB\")\n",
    "                img = img.convert('RGB')\n",
    "            \n",
    "            # Resize if needed\n",
    "            if max(img.size) > max(max_size):\n",
    "                logging.debug(f\"Resizing image from {img.size} to max {max_size}\")\n",
    "                img.thumbnail(max_size)  # Resize while maintaining aspect ratio\n",
    "            \n",
    "            # Save to bytes with specified format\n",
    "            buffer = io.BytesIO()\n",
    "            img.save(buffer, format=target_format, quality=quality)\n",
    "            image_bytes = buffer.getvalue()\n",
    "            \n",
    "            # Get the correct MIME type\n",
    "            mime_type = f\"image/{target_format.lower()}\"\n",
    "            logging.debug(f\"Using MIME type: {mime_type}\")\n",
    "            logging.debug(f\"Image processing successful. Size: {len(image_bytes)} bytes\")\n",
    "            \n",
    "            return mime_type, image_bytes\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing image {image_path}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASL Prediction Function\n",
    "\n",
    "Define the main function for getting ASL predictions from Gemini model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_asl_prediction(image_path: str, prompt_strategy: Literal[\"zero_shot\", \"few_shot\", \"chain_of_thought\", \"visual_grounding\", \"contrastive\"] = \"zero_shot\") -> dict:\n",
    "    \"\"\"Get ASL prediction from Gemini 2.0 Flash-Lite model for a given image path, including visibility check.\"\"\"\n",
    "    config = MODEL_CONFIGS[\"flash-lite\"]\n",
    "    model_name = config[\"name\"]\n",
    "    display_name = config[\"display_name\"]\n",
    "    delay_seconds = config[\"rate_limit_delay\"]\n",
    "    max_retries = config[\"max_retries\"]\n",
    "    \n",
    "    # Initialize timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Process the image and get raw bytes and MIME type\n",
    "        mime_type, image_bytes = encode_and_convert_image(image_path)\n",
    "        logging.debug(f\"Image processed successfully as {mime_type}. Size: {len(image_bytes)} bytes\")\n",
    "\n",
    "        # Step 1: Visibility Check\n",
    "        logging.debug(f\"Sending visibility check request to {display_name}...\")\n",
    "        visibility_contents = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [\n",
    "                    types.Part.from_bytes(\n",
    "                        mime_type=mime_type,\n",
    "                        data=image_bytes\n",
    "                    ),\n",
    "                    {\n",
    "                        \"text\": \"Can you see the image I'm sending? Please respond with ONLY 'yes' or 'no'.\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Count tokens for visibility check\n",
    "        visibility_tokens = client.models.count_tokens(\n",
    "            model=model_name,\n",
    "            contents=visibility_contents\n",
    "        )\n",
    "        \n",
    "        visibility_response = client.models.generate_content(\n",
    "            model=model_name,\n",
    "            contents=visibility_contents,\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=0.05,\n",
    "                top_p=1.0,\n",
    "                max_output_tokens=300\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        visibility_text = visibility_response.text.strip().lower()\n",
    "        logging.debug(f\"Visibility response from {display_name}: {visibility_text}\")\n",
    "        if \"yes\" not in visibility_text:\n",
    "            logging.warning(f\"{display_name} visibility check failed for {image_path}. Response: {visibility_text}\")\n",
    "            return {\n",
    "                \"error\": \"Model cannot see the image\",\n",
    "                \"metadata\": {\n",
    "                    \"response_time_seconds\": round(time.time() - start_time, 3),\n",
    "                    \"visibility_check_tokens\": visibility_tokens.total_tokens\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        logging.debug(f\"{display_name} confirmed image visibility.\")\n",
    "\n",
    "        # Step 2: Proceed with ASL recognition\n",
    "        logging.debug(f\"Sending ASL recognition request to {display_name}...\")\n",
    "        \n",
    "        # Get appropriate prompt template\n",
    "        prompt = PROMPT_TEMPLATES[prompt_strategy]\n",
    "        \n",
    "        # Prepare contents for ASL recognition\n",
    "        asl_contents = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [\n",
    "                    types.Part.from_bytes(\n",
    "                        mime_type=mime_type,\n",
    "                        data=image_bytes\n",
    "                    ),\n",
    "                    {\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Count tokens for ASL recognition\n",
    "        asl_tokens = client.models.count_tokens(\n",
    "            model=model_name,\n",
    "            contents=asl_contents\n",
    "        )\n",
    "        \n",
    "        # Make prediction\n",
    "        response = client.models.generate_content(\n",
    "            model=model_name,\n",
    "            contents=asl_contents,\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=0.05,\n",
    "                top_p=1.0,\n",
    "                max_output_tokens=300\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Calculate total response time\n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        # Extract the response text\n",
    "        response_text = response.text.strip()\n",
    "        logging.debug(f\"Raw ASL response from {display_name}: {response_text}\")\n",
    "        \n",
    "        # Try to parse the JSON response\n",
    "        try:\n",
    "            # Find the JSON object in the response\n",
    "            json_start = response_text.find('{')\n",
    "            json_end = response_text.rfind('}') + 1\n",
    "            if json_start >= 0 and json_end > json_start:\n",
    "                json_str = response_text[json_start:json_end]\n",
    "                result = json.loads(json_str)\n",
    "                \n",
    "                # Add timing and token information to the result\n",
    "                result[\"metadata\"] = {\n",
    "                    \"response_time_seconds\": round(response_time, 3),\n",
    "                    \"visibility_check_tokens\": visibility_tokens.total_tokens,\n",
    "                    \"asl_recognition_tokens\": asl_tokens.total_tokens,\n",
    "                    \"total_tokens\": visibility_tokens.total_tokens + asl_tokens.total_tokens\n",
    "                }\n",
    "                return result\n",
    "            else:\n",
    "                logging.warning(f\"No JSON object found in {display_name} ASL response for {image_path}\")\n",
    "                return {\n",
    "                    \"error\": \"No JSON found in response\",\n",
    "                    \"metadata\": {\n",
    "                        \"response_time_seconds\": round(response_time, 3),\n",
    "                        \"visibility_check_tokens\": visibility_tokens.total_tokens,\n",
    "                        \"asl_recognition_tokens\": asl_tokens.total_tokens,\n",
    "                        \"total_tokens\": visibility_tokens.total_tokens + asl_tokens.total_tokens\n",
    "                    }\n",
    "                }\n",
    "        except json.JSONDecodeError:\n",
    "            logging.warning(f\"Failed to parse JSON from {display_name} ASL response for {image_path}\")\n",
    "            return {\n",
    "                \"error\": \"Invalid JSON response\",\n",
    "                \"metadata\": {\n",
    "                    \"response_time_seconds\": round(response_time, 3),\n",
    "                    \"visibility_check_tokens\": visibility_tokens.total_tokens,\n",
    "                    \"asl_recognition_tokens\": asl_tokens.total_tokens,\n",
    "                    \"total_tokens\": visibility_tokens.total_tokens + asl_tokens.total_tokens\n",
    "                }\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        response_time = time.time() - start_time\n",
    "        logging.error(f\"Error getting prediction from {display_name} for {image_path}: {e}\")\n",
    "        # Check for specific API errors (like google.api_core.exceptions.InvalidArgument)\n",
    "        error_response = {\n",
    "            \"error\": f\"API Error: {e.message}\" if hasattr(e, 'message') else str(e),\n",
    "            \"metadata\": {\n",
    "                \"response_time_seconds\": round(response_time, 3)\n",
    "            }\n",
    "        }\n",
    "        return error_response\n",
    "    finally:\n",
    "        # Add a delay to respect rate limits\n",
    "        logging.debug(f\"Waiting {delay_seconds}s before next {display_name} request...\")\n",
    "        time.sleep(delay_seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Sample Image\n",
    "\n",
    "Test the model with a sample image using different prompting strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this path to your image\n",
    "base_path = Path(\"/Users/henrikjacobsen/Desktop/CBS/Semester 2/Artifical Intelligence and Machine Learning/Final Project/AIML25-shared/model_comparison\")\n",
    "image_path = base_path / \"data/V/V_0_20250428_114109.jpg\"\n",
    "\n",
    "# Make sure the path exists\n",
    "if not image_path.exists():\n",
    "    print(f\"Image not found: {image_path}\")\n",
    "    # Try to find any image in the dataset\n",
    "    data_dir = base_path / \"data\"\n",
    "    if data_dir.exists():\n",
    "        for letter_dir in data_dir.glob(\"*\"):\n",
    "            if letter_dir.is_dir():\n",
    "                for img_file in letter_dir.glob(\"*.jpg\"):\n",
    "                    image_path = img_file\n",
    "                    print(f\"Using alternative image: {image_path}\")\n",
    "                    break\n",
    "            if image_path.exists():\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a specific strategy\n",
    "def test_single_strategy(strategy=\"zero_shot\"):\n",
    "    \"\"\"Test the model with a single prompting strategy.\"\"\"\n",
    "    print(f\"\\nTesting with {strategy} strategy...\")\n",
    "    config = MODEL_CONFIGS[\"flash-lite\"]\n",
    "    print(f\"Using {config['display_name']} on image: {image_path}\")\n",
    "    \n",
    "    result = get_asl_prediction(str(image_path), strategy)\n",
    "    print(f\"\\nResult:\\n{json.dumps(result, indent=2)}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with zero_shot strategy...\n",
      "Using Gemini 2.0 Flash-Lite on image: /Users/henrikjacobsen/Desktop/CBS/Semester 2/Artifical Intelligence and Machine Learning/Final Project/AIML25-shared/model_comparison/data/V/V_0_20250428_114109.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 11:50:40,084 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-lite:countTokens \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 11:50:40,086 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-13 11:50:41,015 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 11:50:41,017 - INFO - AFC remote call 1 is done.\n",
      "2025-05-13 11:50:41,077 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-lite:countTokens \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 11:50:41,078 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-13 11:50:42,060 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 11:50:42,062 - INFO - AFC remote call 1 is done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result:\n",
      "{\n",
      "  \"letter\": \"V\",\n",
      "  \"confidence\": 0.95,\n",
      "  \"feedback\": \"The index and middle fingers are extended and separated, while the other fingers are closed, forming the letter V in ASL.\",\n",
      "  \"metadata\": {\n",
      "    \"response_time_seconds\": 2.133,\n",
      "    \"visibility_check_tokens\": 280,\n",
      "    \"asl_recognition_tokens\": 366,\n",
      "    \"total_tokens\": 646\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Run the test with the zero_shot strategy\n",
    "result = test_single_strategy(\"zero_shot\")\n",
    "\n",
    "# Uncomment to test all strategies\n",
    "# all_results = test_all_strategies()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
