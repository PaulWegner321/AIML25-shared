{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASL Model Evaluation Framework\n",
    "\n",
    "This notebook provides a framework for evaluating multiple machine learning models on American Sign Language (ASL) recognition tasks. It can compare various large vision language models and traditional computer vision models on their ability to recognize ASL hand signs.\n",
    "\n",
    "The framework supports:\n",
    "- Multiple models\n",
    "- Multiple prompting strategies\n",
    "- Various evaluation metrics\n",
    "\n",
    "### Please refer to evaluate_models.py for the actually implemented script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import argparse\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import traceback\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "from sklearn.metrics import confusion_matrix, classification_report, top_k_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, we'll set up logging, import the model predictors, and define our evaluation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 11:24:59,274 - INFO - Logging to file: evaluation_logs/evaluation_20250513_112459.log\n"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_dir = \"evaluation_logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, f\"evaluation_{timestamp}.log\")\n",
    "\n",
    "# Configure logging to write to both file and console\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logging.info(f\"Logging to file: {log_file}\")\n",
    "\n",
    "# Initialize global results dictionary\n",
    "results = {\n",
    "    \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    \"dataset_path\": \"\",\n",
    "    \"sample_size\": 0,\n",
    "    \"results\": {}\n",
    "}\n",
    "\n",
    "# Initialize global variables for token management\n",
    "watsonx_token = None\n",
    "watsonx_token_expiry = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .env file from: /Users/henrikjacobsen/Desktop/CBS/Semester 2/Artifical Intelligence and Machine Learning/Final Project/AIML25-shared/backend/.env\n",
      "Loading .env file from: /Users/henrikjacobsen/Desktop/CBS/Semester 2/Artifical Intelligence and Machine Learning/Final Project/AIML25-shared/backend/.env\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 11:25:01,263 - INFO - Available models: ['gpt4_turbo', 'gpt4o', 'gemini_flash', 'gemini_flash_lite', 'llama_90b', 'llama_maverick', 'llama_scout', 'mistral', 'granite_vision']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .env file from: /Users/henrikjacobsen/Desktop/CBS/Semester 2/Artifical Intelligence and Machine Learning/Final Project/AIML25-shared/backend/.env\n",
      "Loading .env file from: /Users/henrikjacobsen/Desktop/CBS/Semester 2/Artifical Intelligence and Machine Learning/Final Project/AIML25-shared/backend/.env\n",
      "Loading .env file from: /Users/henrikjacobsen/Desktop/CBS/Semester 2/Artifical Intelligence and Machine Learning/Final Project/AIML25-shared/backend/.env\n",
      "Loading .env file from: /Users/henrikjacobsen/Desktop/CBS/Semester 2/Artifical Intelligence and Machine Learning/Final Project/AIML25-shared/backend/.env\n",
      "Loading .env file from: /Users/henrikjacobsen/Desktop/CBS/Semester 2/Artifical Intelligence and Machine Learning/Final Project/AIML25-shared/backend/.env\n",
      "Loading .env file from: /Users/henrikjacobsen/Desktop/CBS/Semester 2/Artifical Intelligence and Machine Learning/Final Project/AIML25-shared/backend/.env\n",
      "Loading .env file from: /Users/henrikjacobsen/Desktop/CBS/Semester 2/Artifical Intelligence and Machine Learning/Final Project/AIML25-shared/backend/.env\n"
     ]
    }
   ],
   "source": [
    "# --- Import Model Predictors --- #\n",
    "\n",
    "# Keep these imports active\n",
    "AVAILABLE_MODELS = {}\n",
    "\n",
    "try:\n",
    "    from test_gpt4_turbo import get_asl_prediction as get_gpt4_turbo_prediction\n",
    "    AVAILABLE_MODELS[\"gpt4_turbo\"] = get_gpt4_turbo_prediction\n",
    "except ImportError as e:\n",
    "    logging.warning(f\"GPT-4 Turbo model not available: {e}\")\n",
    "\n",
    "try:\n",
    "    from test_gpt4o import get_asl_prediction as get_gpt4o_prediction\n",
    "    AVAILABLE_MODELS[\"gpt4o\"] = get_gpt4o_prediction\n",
    "except ImportError as e:\n",
    "    logging.warning(f\"GPT-4 Vision model not available: {e}\")\n",
    "\n",
    "try:\n",
    "    from test_gemini_2_flash import get_asl_prediction as get_gemini_flash_prediction\n",
    "    AVAILABLE_MODELS[\"gemini_flash\"] = get_gemini_flash_prediction\n",
    "except ImportError as e:\n",
    "    logging.warning(f\"Gemini Flash model not available: {e}\")\n",
    "\n",
    "try:\n",
    "    from test_gemini_2_flash_lite import get_asl_prediction as get_gemini_flash_lite_prediction\n",
    "    AVAILABLE_MODELS[\"gemini_flash_lite\"] = get_gemini_flash_lite_prediction\n",
    "except ImportError as e:\n",
    "    logging.warning(f\"Gemini Flash Lite model not available: {e}\")\n",
    "\n",
    "try:\n",
    "    from test_llama_90b_vision import get_asl_prediction as get_llama_90b_prediction\n",
    "    AVAILABLE_MODELS[\"llama_90b\"] = get_llama_90b_prediction\n",
    "except ImportError as e:\n",
    "    logging.warning(f\"Llama 90B Vision model not available: {e}\")\n",
    "\n",
    "try:\n",
    "    from test_llama_maverick_17b import get_asl_prediction as get_llama_maverick_prediction\n",
    "    AVAILABLE_MODELS[\"llama_maverick\"] = get_llama_maverick_prediction\n",
    "except ImportError as e:\n",
    "    logging.warning(f\"Llama Maverick model not available: {e}\")\n",
    "\n",
    "try:\n",
    "    from test_llama_scout_17b import get_asl_prediction as get_llama_scout_prediction\n",
    "    AVAILABLE_MODELS[\"llama_scout\"] = get_llama_scout_prediction\n",
    "except ImportError as e:\n",
    "    logging.warning(f\"Llama Scout model not available: {e}\")\n",
    "\n",
    "try:\n",
    "    from test_pixtral_12b import get_asl_prediction as get_mistral_prediction\n",
    "    AVAILABLE_MODELS[\"mistral\"] = get_mistral_prediction\n",
    "except ImportError as e:\n",
    "    logging.warning(f\"Mistral (Pixtral) model not available: {e}\")\n",
    "\n",
    "try:\n",
    "    from test_granite_vision import get_asl_prediction as get_granite_vision_prediction\n",
    "    AVAILABLE_MODELS[\"granite_vision\"] = get_granite_vision_prediction\n",
    "except ImportError as e:\n",
    "    logging.warning(f\"Granite Vision model not available: {e}\")\n",
    "\n",
    "# Define models to evaluate based on availability\n",
    "MODELS_TO_EVALUATE = list(AVAILABLE_MODELS.keys())\n",
    "logging.info(f\"Available models: {MODELS_TO_EVALUATE}\")\n",
    "\n",
    "if not MODELS_TO_EVALUATE:\n",
    "    logging.warning(\"No models available for evaluation. Please check your environment variables and dependencies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 11:25:01,270 - INFO - Attempting to load .env file from: /Users/henrikjacobsen/Desktop/CBS/Semester 2/Artifical Intelligence and Machine Learning/Final Project/backend/.env\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from backend/.env\n",
    "dotenv_path = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), 'backend', '.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "logging.info(f\"Attempting to load .env file from: {dotenv_path}\")\n",
    "\n",
    "# Define valid classes and class mapping\n",
    "VALID_CLASSES = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")  # All ASL letters\n",
    "CLASS_MAP = {label: i for i, label in enumerate(VALID_CLASSES)}\n",
    "INDEX_TO_CLASS = {i: label for label, i in CLASS_MAP.items()}\n",
    "\n",
    "# Define available prompt strategies (use all 5)\n",
    "PROMPT_STRATEGIES = [\n",
    "    \"zero_shot\",\n",
    "    \"few_shot\",\n",
    "    \"chain_of_thought\",\n",
    "    \"visual_grounding\",\n",
    "    \"contrastive\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "We'll define helper functions for authentication, image processing, and model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_watsonx_token(api_key):\n",
    "    \"\"\"Gets or refreshes the IBM Cloud IAM token.\"\"\"\n",
    "    global watsonx_token, watsonx_token_expiry\n",
    "    now = time.time()\n",
    "\n",
    "    if watsonx_token and watsonx_token_expiry and now < watsonx_token_expiry:\n",
    "        return watsonx_token\n",
    "\n",
    "    logging.info(\"Refreshing WatsonX authentication token...\")\n",
    "    auth_url = \"https://iam.cloud.ibm.com/identity/token\"\n",
    "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "    data = {\"grant_type\": \"urn:ibm:params:oauth:grant-type:apikey\", \"apikey\": api_key}\n",
    "    try:\n",
    "        response = requests.post(auth_url, headers=headers, data=data, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        token_data = response.json()\n",
    "        watsonx_token = token_data.get(\"access_token\")\n",
    "        expires_in = token_data.get(\"expires_in\", 3600)\n",
    "        watsonx_token_expiry = now + expires_in - 300  # 5 min buffer\n",
    "        return watsonx_token\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Authentication failed: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error refreshing token: {e}\")\n",
    "        return None\n",
    "\n",
    "def encode_image_base64(image_path, resize_dim=(256, 256)):\n",
    "    \"\"\"Reads, resizes, and returns the base64 encoded string of an image.\"\"\"\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "\n",
    "            if resize_dim:\n",
    "                img = img.resize(resize_dim)\n",
    "\n",
    "            buffer = io.BytesIO()\n",
    "            img.save(buffer, format=\"JPEG\", quality=95)\n",
    "            base64_str = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "            base64_str = ''.join(base64_str.split())\n",
    "            return base64_str\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error encoding/resizing image {image_path}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_sample(dataset_path_str, sample_size=30):\n",
    "    \"\"\"Load random samples from the dataset for each letter.\"\"\"\n",
    "    dataset_path = Path(dataset_path_str)\n",
    "    if not dataset_path.exists():\n",
    "        raise ValueError(f\"Dataset path {dataset_path} does not exist\")\n",
    "    \n",
    "    samples = []\n",
    "    for letter in VALID_CLASSES:\n",
    "        letter_dir = dataset_path / letter\n",
    "        if not letter_dir.exists():\n",
    "            logging.warning(f\"Directory for letter {letter} not found: {letter_dir}\")\n",
    "            continue\n",
    "            \n",
    "        # Get all image files in the directory\n",
    "        image_files = list(letter_dir.glob(\"*.jpg\"))\n",
    "        if not image_files:\n",
    "            logging.warning(f\"No images found for letter {letter} in {letter_dir}\")\n",
    "            continue\n",
    "            \n",
    "        # Randomly sample images\n",
    "        if len(image_files) < sample_size:\n",
    "            logging.warning(f\"Not enough images for letter {letter}. Found {len(image_files)}, requested {sample_size}\")\n",
    "            selected_images = image_files\n",
    "        else:\n",
    "            selected_images = random.sample(image_files, sample_size)\n",
    "            \n",
    "        # Add selected images to samples list\n",
    "        for img_path in selected_images:\n",
    "            samples.append((str(img_path), letter))\n",
    "    \n",
    "    logging.info(f\"Loaded {len(samples)} images across all letters\")\n",
    "    return samples\n",
    "\n",
    "def get_prediction(model_name, image_path, prompt_strategy=\"zero_shot\"):\n",
    "    \"\"\"Get prediction from the specified model.\"\"\"\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        if model_name not in AVAILABLE_MODELS:\n",
    "            raise ValueError(f\"Model {model_name} is not available\")\n",
    "            \n",
    "        # Get the prediction function for this model\n",
    "        predict_func = AVAILABLE_MODELS[model_name]\n",
    "        \n",
    "        # Handle different parameter naming conventions\n",
    "        if model_name in [\"gemini_flash\", \"gemini_flash_lite\", \"granite_vision\"]:\n",
    "            result = predict_func(image_path, prompt_strategy=prompt_strategy)\n",
    "        elif model_name in [\"llama_90b\", \"llama_maverick\", \"llama_scout\", \"mistral\"]:\n",
    "            result = predict_func(image_path, strategy=prompt_strategy)\n",
    "        else:  # GPT-4 models\n",
    "            result = predict_func(image_path, prompt_strategy=prompt_strategy)\n",
    "        \n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        # Extract token usage if available\n",
    "        token_usage = 0\n",
    "        if isinstance(result, dict):\n",
    "            if \"metadata\" in result:\n",
    "                if \"total_tokens\" in result[\"metadata\"]:\n",
    "                    token_usage = result[\"metadata\"][\"total_tokens\"]\n",
    "                elif \"tokens\" in result[\"metadata\"] and \"total\" in result[\"metadata\"][\"tokens\"]:\n",
    "                    token_usage = result[\"metadata\"][\"tokens\"][\"total\"]\n",
    "            \n",
    "            # If the result has a \"prediction\" key, use that as the actual result\n",
    "            if \"prediction\" in result:\n",
    "                result = result[\"prediction\"]\n",
    "        \n",
    "        return result, response_time, token_usage\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error getting prediction from {model_name}: {str(e)}\")\n",
    "        response_time = time.time() - start_time\n",
    "        return {\"error\": str(e)}, response_time, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_result(model_name, prediction_result, true_letter, image_path, response_time=None, token_usage=None, prompt_strategy=\"zero_shot\"):\n",
    "    \"\"\"Handle prediction result and update global results.\"\"\"\n",
    "    if model_name not in results[\"results\"]:\n",
    "        results[\"results\"][model_name] = {\n",
    "            \"predictions\": [],\n",
    "            \"ground_truth\": [],\n",
    "            \"response_times\": [],\n",
    "            \"token_usage\": [],\n",
    "            \"prompt_strategy_results\": {}\n",
    "        }\n",
    "    \n",
    "    # Add to overall results\n",
    "    results[\"results\"][model_name][\"predictions\"].append(prediction_result)\n",
    "    results[\"results\"][model_name][\"ground_truth\"].append(true_letter)\n",
    "    results[\"results\"][model_name][\"response_times\"].append(response_time if response_time else 0)\n",
    "    results[\"results\"][model_name][\"token_usage\"].append(token_usage if token_usage else 0)\n",
    "    \n",
    "    # Add to strategy-specific results\n",
    "    if prompt_strategy not in results[\"results\"][model_name][\"prompt_strategy_results\"]:\n",
    "        results[\"results\"][model_name][\"prompt_strategy_results\"][prompt_strategy] = {\n",
    "            \"predictions\": [],\n",
    "            \"ground_truth\": [],\n",
    "            \"response_times\": [],\n",
    "            \"token_usage\": []\n",
    "        }\n",
    "    \n",
    "    strategy_data = results[\"results\"][model_name][\"prompt_strategy_results\"][prompt_strategy]\n",
    "    strategy_data[\"predictions\"].append(prediction_result)\n",
    "    strategy_data[\"ground_truth\"].append(true_letter)\n",
    "    strategy_data[\"response_times\"].append(response_time if response_time else 0)\n",
    "    strategy_data[\"token_usage\"].append(token_usage if token_usage else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Calculation and Visualization\n",
    "\n",
    "These functions calculate evaluation metrics and generate visualizations for the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(results: Dict[str, Any], misclassified: Dict[str, List[Tuple[str, str]]]) -> Dict[str, Any]:\n",
    "    \"\"\"Calculate statistics for each model and strategy.\"\"\"\n",
    "    for model_name, model_data in results[\"results\"].items():\n",
    "        # Calculate overall metrics\n",
    "        predictions = model_data[\"predictions\"]\n",
    "        ground_truth = model_data[\"ground_truth\"]\n",
    "        response_times = model_data[\"response_times\"]\n",
    "        token_usage = model_data[\"token_usage\"]\n",
    "        \n",
    "        # Calculate overall statistics\n",
    "        correct = sum(1 for p, g in zip(predictions, ground_truth) if p == g)\n",
    "        total = len(predictions)\n",
    "        errors = total - correct\n",
    "        \n",
    "        model_data.update({\n",
    "            \"correct\": correct,\n",
    "            \"total\": total,\n",
    "            \"errors\": errors,\n",
    "            \"metrics\": calculate_metrics(predictions, ground_truth)\n",
    "        })\n",
    "        \n",
    "        # Calculate strategy-specific statistics\n",
    "        for strategy, strategy_data in model_data[\"prompt_strategy_results\"].items():\n",
    "            strategy_predictions = strategy_data[\"predictions\"]\n",
    "            strategy_ground_truth = strategy_data[\"ground_truth\"]\n",
    "            strategy_response_times = strategy_data[\"response_times\"]\n",
    "            strategy_token_usage = strategy_data[\"token_usage\"]\n",
    "            \n",
    "            strategy_correct = sum(1 for p, g in zip(strategy_predictions, strategy_ground_truth) if p == g)\n",
    "            strategy_total = len(strategy_predictions)\n",
    "            \n",
    "            strategy_data.update({\n",
    "                \"correct\": strategy_correct,\n",
    "                \"total\": strategy_total,\n",
    "                \"metrics\": calculate_metrics(strategy_predictions, strategy_ground_truth)\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def calculate_metrics(predictions: List[str], ground_truth: List[str], prediction_probs: np.ndarray = None) -> Dict[str, Any]:\n",
    "    \"\"\"Calculate comprehensive evaluation metrics.\"\"\"\n",
    "    if not predictions or not ground_truth:\n",
    "        return {\"error\": \"No predictions or ground truth data available\"}\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Filter out invalid predictions (ensure they're all strings and within VALID_CLASSES)\n",
    "    valid_indices = []\n",
    "    for i, (pred, truth) in enumerate(zip(predictions, ground_truth)):\n",
    "        if isinstance(pred, str) and isinstance(truth, str) and pred in VALID_CLASSES and truth in VALID_CLASSES:\n",
    "            valid_indices.append(i)\n",
    "    \n",
    "    if not valid_indices:\n",
    "        return {\"error\": \"No valid predictions available for metrics calculation\"}\n",
    "    \n",
    "    # Use only the valid predictions and ground truth values\n",
    "    filtered_preds = [predictions[i] for i in valid_indices]\n",
    "    filtered_truth = [ground_truth[i] for i in valid_indices]\n",
    "    \n",
    "    # Convert to numpy arrays for easier calculations\n",
    "    preds = np.array(filtered_preds)\n",
    "    truth = np.array(filtered_truth)\n",
    "    \n",
    "    # 1. Basic Accuracy\n",
    "    metrics[\"accuracy\"] = np.mean(preds == truth)\n",
    "    \n",
    "    # 2. Confusion Matrix (if we have enough data)\n",
    "    try:\n",
    "        cm = confusion_matrix(truth, preds, labels=VALID_CLASSES)\n",
    "        metrics[\"confusion_matrix\"] = cm.tolist()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error calculating confusion matrix: {e}\")\n",
    "        metrics[\"confusion_matrix\"] = None\n",
    "    \n",
    "    # 3. Classification Report (if we have enough data)\n",
    "    try:\n",
    "        report = classification_report(truth, preds, labels=VALID_CLASSES, output_dict=True)\n",
    "        metrics[\"classification_report\"] = report\n",
    "        \n",
    "        # Extract macro and weighted averages\n",
    "        metrics[\"macro_avg\"] = {\n",
    "            \"precision\": report[\"macro avg\"][\"precision\"],\n",
    "            \"recall\": report[\"macro avg\"][\"recall\"],\n",
    "            \"f1_score\": report[\"macro avg\"][\"f1-score\"]\n",
    "        }\n",
    "        \n",
    "        metrics[\"weighted_avg\"] = {\n",
    "            \"precision\": report[\"weighted avg\"][\"precision\"],\n",
    "            \"recall\": report[\"weighted avg\"][\"recall\"],\n",
    "            \"f1_score\": report[\"weighted avg\"][\"f1-score\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error calculating classification report: {e}\")\n",
    "        metrics[\"classification_report\"] = None\n",
    "        metrics[\"macro_avg\"] = None\n",
    "        metrics[\"weighted_avg\"] = None\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_subset(predictions: List[str], ground_truth: List[str], image_paths: List[str], subset_type: str) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate model performance on a specific subset of images.\"\"\"\n",
    "    subset_indices = []\n",
    "    for i, path in enumerate(image_paths):\n",
    "        if subset_type == 'grayscale' and 'grayscale' in path.lower():\n",
    "            subset_indices.append(i)\n",
    "        elif subset_type == 'flipped' and 'flipped' in path.lower():\n",
    "            subset_indices.append(i)\n",
    "    \n",
    "    if not subset_indices:\n",
    "        return {\n",
    "            \"count\": 0,\n",
    "            \"accuracy\": None,\n",
    "            \"error_rate\": None\n",
    "        }\n",
    "    \n",
    "    # Only include indices that have corresponding predictions\n",
    "    valid_indices = [i for i in subset_indices if i < len(predictions)]\n",
    "    \n",
    "    if not valid_indices:\n",
    "        return {\n",
    "            \"count\": 0,\n",
    "            \"accuracy\": None,\n",
    "            \"error_rate\": None\n",
    "        }\n",
    "    \n",
    "    subset_preds = [predictions[i] for i in valid_indices]\n",
    "    subset_truth = [ground_truth[i] for i in valid_indices]\n",
    "    \n",
    "    accuracy = np.mean(np.array(subset_preds) == np.array(subset_truth))\n",
    "    error_rate = 1 - accuracy\n",
    "    \n",
    "    return {\n",
    "        \"count\": len(valid_indices),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"error_rate\": error_rate\n",
    "    }\n",
    "\n",
    "def save_intermediate_results(output_dir: str, model_name: str) -> None:\n",
    "    \"\"\"Save intermediate results to a temporary file.\"\"\"\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    temp_file = os.path.join(output_dir, f\"temp_results_{model_name}_{timestamp}.json\")\n",
    "    with open(temp_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    logging.info(f\"Saved intermediate results to {temp_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, labels, title, output_path):\n",
    " \"\"\"Plot and save a confusion matrix as a heatmap.\"\"\"\n",
    " plt.figure(figsize=(12, 10))\n",
    " \n",
    " # Normalize the confusion matrix for display\n",
    " cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    " cm_normalized = np.nan_to_num(cm_normalized) # Replace NaN with 0\n",
    " \n",
    " # Create the heatmap\n",
    " sns.heatmap(cm_normalized, annot=cm, fmt='d', cmap='Blues',\n",
    " xticklabels=labels, yticklabels=labels)\n",
    " \n",
    " plt.title(title)\n",
    " plt.ylabel('True Label')\n",
    " plt.xlabel('Predicted Label')\n",
    " \n",
    " # Save the figure\n",
    " plt.tight_layout()\n",
    " plt.savefig(output_path, dpi=300)\n",
    " plt.close()\n",
    " \n",
    " return output_path\n",
    "\n",
    "def generate_confusion_matrices(data, output_dir):\n",
    "    \"\"\"Generate confusion matrices for each model and strategy.\"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all possible labels (A-Z)\n",
    "    labels = list(string.ascii_uppercase)\n",
    "    \n",
    "    for model_name, model_data in data['results'].items():\n",
    "        # Check if we have prediction data\n",
    "        if 'predictions' not in model_data or not model_data['predictions']:\n",
    "            logging.warning(f\"No prediction data for model {model_name}\")\n",
    "            continue\n",
    "            \n",
    "        # Overall model confusion matrix\n",
    "        try:\n",
    "            cm = confusion_matrix(\n",
    "                model_data['ground_truth'],\n",
    "                model_data['predictions'],\n",
    "                labels=labels\n",
    "            )\n",
    "            plot_confusion_matrix(\n",
    "                cm, \n",
    "                labels,\n",
    "                f'Confusion Matrix - {model_name}',\n",
    "                os.path.join(output_dir, f'confusion_matrix_{model_name.lower()}.png')\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating confusion matrix for {model_name}: {e}\")\n",
    "        \n",
    "        # Strategy-specific confusion matrices\n",
    "        if 'prompt_strategy_results' in model_data:\n",
    "            for strategy, strategy_data in model_data['prompt_strategy_results'].items():\n",
    "                if not strategy_data.get('predictions'):\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    cm = confusion_matrix(\n",
    "                        strategy_data['ground_truth'],\n",
    "                        strategy_data['predictions'],\n",
    "                        labels=labels\n",
    "                    )\n",
    "                    plot_confusion_matrix(\n",
    "                        cm,\n",
    "                        labels,\n",
    "                        f'Confusion Matrix - {model_name} - {strategy}',\n",
    "                        os.path.join(output_dir, f'confusion_matrix_{model_name.lower()}_{strategy.lower()}.png')\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error generating confusion matrix for {model_name} - {strategy}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Evaluation Function\n",
    "\n",
    "This section defines the main evaluation function that will run the evaluation on all models using the specified prompting strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 11:27:51,408 - INFO - Evaluating model: gpt4_turbo\n",
      "2025-05-13 11:27:51,408 - INFO - Using prompt strategy: zero_shot\n",
      "gpt4_turbo - zero_shot:   0%|          | 0/1 [00:00<?, ?it/s]2025-05-13 11:27:51,418 - INFO - Image encoded successfully. Base64 size: 49920 characters\n",
      "2025-05-13 11:27:55,773 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 11:27:55,776 - INFO - Generated text: {\n",
      "  \"letter\": \"V\",\n",
      "  \"confidence\": 0.95,\n",
      "  \"feedback\": \"The gesture shows the person holding up their hand with the index and middle fingers raised and separated, which corresponds to the ASL sign for the letter 'V'.\"\n",
      "}\n",
      "gpt4_turbo - zero_shot: 100%|██████████| 1/1 [00:04<00:00,  4.37s/it]\n",
      "2025-05-13 11:27:55,780 - INFO - Saved intermediate results to evaluation_results/temp_results_gpt4_turbo_zero_shot_20250513_112755.json\n",
      "2025-05-13 11:27:55,781 - INFO - Evaluating model: gpt4o\n",
      "2025-05-13 11:27:55,782 - INFO - Using prompt strategy: zero_shot\n",
      "gpt4o - zero_shot:   0%|          | 0/1 [00:00<?, ?it/s]2025-05-13 11:27:55,791 - INFO - Image encoded successfully. Base64 size: 49920 characters\n",
      "2025-05-13 11:27:58,639 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 11:27:58,641 - INFO - Generated text: ```json\n",
      "{\n",
      "  \"letter\": \"U\",\n",
      "  \"confidence\": 0.9,\n",
      "  \"feedback\": \"The gesture shows two fingers extended and together, with the rest of the fingers folded down, which corresponds to the ASL letter 'U'.\"\n",
      "}\n",
      "```\n",
      "gpt4o - zero_shot: 100%|██████████| 1/1 [00:02<00:00,  2.86s/it]\n",
      "2025-05-13 11:27:58,646 - INFO - Saved intermediate results to evaluation_results/temp_results_gpt4o_zero_shot_20250513_112758.json\n",
      "2025-05-13 11:27:58,646 - INFO - Evaluating model: gemini_flash\n",
      "2025-05-13 11:27:58,647 - INFO - Using prompt strategy: zero_shot\n",
      "gemini_flash - zero_shot:   0%|          | 0/1 [00:00<?, ?it/s]2025-05-13 11:27:58,756 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:countTokens \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 11:27:58,758 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-13 11:27:59,686 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 11:27:59,688 - INFO - AFC remote call 1 is done.\n",
      "2025-05-13 11:27:59,736 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:countTokens \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 11:27:59,738 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-13 11:28:00,688 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 11:28:00,690 - INFO - AFC remote call 1 is done.\n",
      "gemini_flash - zero_shot: 100%|██████████| 1/1 [00:07<00:00,  7.04s/it]\n",
      "2025-05-13 11:28:05,696 - INFO - Saved intermediate results to evaluation_results/temp_results_gemini_flash_zero_shot_20250513_112805.json\n",
      "2025-05-13 11:28:05,697 - INFO - Evaluating model: gemini_flash_lite\n",
      "2025-05-13 11:28:05,698 - INFO - Using prompt strategy: zero_shot\n",
      "gemini_flash_lite - zero_shot:   0%|          | 0/1 [00:00<?, ?it/s]2025-05-13 11:28:05,816 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-lite:countTokens \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 11:28:05,820 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-13 11:28:06,694 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 11:28:06,697 - INFO - AFC remote call 1 is done.\n",
      "2025-05-13 11:28:06,751 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-lite:countTokens \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 11:28:06,753 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-13 11:28:07,682 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 11:28:07,685 - INFO - AFC remote call 1 is done.\n",
      "gemini_flash_lite - zero_shot: 100%|██████████| 1/1 [00:04<00:00,  4.99s/it]\n",
      "2025-05-13 11:28:10,696 - INFO - Saved intermediate results to evaluation_results/temp_results_gemini_flash_lite_zero_shot_20250513_112810.json\n",
      "2025-05-13 11:28:10,697 - INFO - Evaluating model: llama_90b\n",
      "2025-05-13 11:28:10,698 - INFO - Using prompt strategy: zero_shot\n",
      "llama_90b - zero_shot:   0%|          | 0/1 [00:00<?, ?it/s]2025-05-13 11:28:11,250 - INFO - Image encoded successfully. Base64 size: 49920 characters\n",
      "2025-05-13 11:28:39,527 - INFO - Generated text: {\n",
      "  \"letter\": \"L\",\n",
      "  \"confidence\": 0.8,\n",
      "  \"feedback\": \"The handshape and orientation of the fingers match the ASL letter L, with the thumb and index finger extended and the other fingers curled in.\"\n",
      "}\n",
      "llama_90b - zero_shot: 100%|██████████| 1/1 [00:28<00:00, 28.83s/it]\n",
      "2025-05-13 11:28:39,533 - INFO - Saved intermediate results to evaluation_results/temp_results_llama_90b_zero_shot_20250513_112839.json\n",
      "2025-05-13 11:28:39,534 - INFO - Evaluating model: llama_maverick\n",
      "2025-05-13 11:28:39,535 - INFO - Using prompt strategy: zero_shot\n",
      "llama_maverick - zero_shot:   0%|          | 0/1 [00:00<?, ?it/s]2025-05-13 11:28:40,125 - INFO - Image encoded successfully. Base64 size: 86896 characters\n",
      "2025-05-13 11:28:40,125 - INFO - Testing image visibility...\n",
      "2025-05-13 11:28:41,036 - INFO - Visibility test response: Yes, I can see the image\n",
      "2025-05-13 11:28:41,037 - INFO - Proceeding with ASL sign recognition...\n",
      "2025-05-13 11:28:42,163 - INFO - Generated text: {\"letter\": \"D\", \"confidence\": \"0.9\", \"feedback\": \"The handshape and orientation match the ASL letter 'D', with the index finger extended and other fingers closed.\"}\n",
      "llama_maverick - zero_shot: 100%|██████████| 1/1 [00:02<00:00,  2.63s/it]\n",
      "2025-05-13 11:28:42,169 - INFO - Saved intermediate results to evaluation_results/temp_results_llama_maverick_zero_shot_20250513_112842.json\n",
      "2025-05-13 11:28:42,170 - INFO - Evaluating model: llama_scout\n",
      "2025-05-13 11:28:42,171 - INFO - Using prompt strategy: zero_shot\n",
      "llama_scout - zero_shot:   0%|          | 0/1 [00:00<?, ?it/s]2025-05-13 11:28:42,468 - INFO - Original image dimensions: (640, 480)\n",
      "2025-05-13 11:28:42,481 - INFO - Resized image dimensions: (512, 512)\n",
      "2025-05-13 11:28:42,486 - INFO - Buffer size before base64: 37013 bytes\n",
      "2025-05-13 11:28:42,487 - INFO - Base64 string length: 49352\n",
      "llama_scout - zero_shot: 100%|██████████| 1/1 [00:03<00:00,  3.41s/it]\n",
      "2025-05-13 11:28:45,590 - INFO - Saved intermediate results to evaluation_results/temp_results_llama_scout_zero_shot_20250513_112845.json\n",
      "2025-05-13 11:28:45,591 - INFO - Evaluating model: mistral\n",
      "2025-05-13 11:28:45,592 - INFO - Using prompt strategy: zero_shot\n",
      "mistral - zero_shot:   0%|          | 0/1 [00:00<?, ?it/s]2025-05-13 11:28:45,781 - INFO - Image encoded successfully. Base64 size: 46980 characters\n",
      "2025-05-13 11:28:45,781 - INFO - Testing image visibility...\n",
      "2025-05-13 11:28:46,842 - INFO - Visibility test response: Yes, I can see a hand gesture in the image. The person is holding up their hand with the index finger and thumb forming a \"C\" shape, often used to signify \"peace\" or \"victory.\"\n",
      "2025-05-13 11:28:46,842 - INFO - Proceeding with ASL sign recognition...\n",
      "2025-05-13 11:28:47,971 - INFO - Generated text: ```json\n",
      "{\n",
      "  \"letter\": \"F\",\n",
      "  \"confidence\": \"0.9\",\n",
      "  \"feedback\": \"The hand is positioned with the palm facing outwards and fingers extended, which corresponds to the ASL letter 'F'.\"\n",
      "}\n",
      "```\n",
      "mistral - zero_shot: 100%|██████████| 1/1 [00:02<00:00,  2.38s/it]\n",
      "2025-05-13 11:28:47,975 - INFO - Saved intermediate results to evaluation_results/temp_results_mistral_zero_shot_20250513_112847.json\n",
      "2025-05-13 11:28:47,976 - INFO - Evaluating model: granite_vision\n",
      "2025-05-13 11:28:47,976 - INFO - Using prompt strategy: zero_shot\n",
      "granite_vision - zero_shot:   0%|          | 0/1 [00:00<?, ?it/s]2025-05-13 11:28:48,204 - INFO - Image data URI format: data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBA...\n",
      "granite_vision - zero_shot: 100%|██████████| 1/1 [00:03<00:00,  3.46s/it]\n",
      "2025-05-13 11:28:51,439 - INFO - Saved intermediate results to evaluation_results/temp_results_granite_vision_zero_shot_20250513_112851.json\n"
     ]
    }
   ],
   "source": [
    "def evaluate_models(dataset_path: str, sample_size: int = 30, output_dir: str = \"evaluation_results\") -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate all available models on the dataset.\"\"\"\n",
    "    # Update global results with dataset info\n",
    "    results[\"dataset_path\"] = dataset_path\n",
    "    results[\"sample_size\"] = sample_size\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load dataset samples\n",
    "    samples = load_dataset_sample(dataset_path, sample_size)\n",
    "    \n",
    "    # Evaluate each model\n",
    "    for model_name in MODELS_TO_EVALUATE:\n",
    "        logging.info(f\"Evaluating model: {model_name}\")\n",
    "        \n",
    "        # Evaluate each prompt strategy\n",
    "        for strategy in PROMPT_STRATEGIES:\n",
    "            logging.info(f\"Using prompt strategy: {strategy}\")\n",
    "            \n",
    "            # Evaluate each sample\n",
    "            for image_path, true_letter in tqdm(samples, desc=f\"{model_name} - {strategy}\"):\n",
    "                try:\n",
    "                    # Get prediction with timing\n",
    "                    start_time = time.time()\n",
    "                    prediction_result, response_time, token_usage = get_prediction(model_name, image_path, strategy)\n",
    "                    \n",
    "                    # Handle the result\n",
    "                    handle_result(\n",
    "                        model_name,\n",
    "                        prediction_result,\n",
    "                        true_letter,\n",
    "                        image_path,\n",
    "                        response_time,\n",
    "                        token_usage,\n",
    "                        strategy\n",
    "                    )\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error evaluating {model_name} on {image_path}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Save intermediate results after each strategy is complete\n",
    "            save_intermediate_results(output_dir, f\"{model_name}_{strategy}\")\n",
    "    \n",
    "    # Calculate final statistics\n",
    "    final_results = calculate_statistics(results, {})\n",
    "    \n",
    "    # Generate confusion matrices\n",
    "    # generate_confusion_matrices(final_results, output_dir)\n",
    "    \n",
    "    # Save results\n",
    "    output_file = os.path.join(output_dir, f\"evaluation_results_{time.strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(final_results, f, indent=2)\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "def main():\n",
    "    # Define parameters\n",
    "    dataset_path = '/Users/henrikjacobsen/Desktop/CBS/Semester 2/Artifical Intelligence and Machine Learning/Final Project/AIML25-shared/model_comparison/data'\n",
    "    sample_size = 30\n",
    "    output_dir = 'evaluation_results'\n",
    "    quick_test = True  # Set to False for full evaluation\n",
    "\n",
    "    if quick_test:\n",
    "        # For quick test, override the sample loading to get just one random image\n",
    "        def quick_test_sample(dataset_path_str, sample_size=None):\n",
    "            dataset_path = Path(dataset_path_str)\n",
    "            all_images = []\n",
    "            for letter in VALID_CLASSES:\n",
    "                letter_dir = dataset_path / letter\n",
    "                if letter_dir.exists():\n",
    "                    image_files = list(letter_dir.glob(\"*.jpg\"))\n",
    "                    if image_files:\n",
    "                        all_images.append((str(random.choice(image_files)), letter))\n",
    "            if all_images:\n",
    "                return [random.choice(all_images)]\n",
    "            return []\n",
    "        \n",
    "        # # Declare globals before modifying\n",
    "        global PROMPT_STRATEGIES\n",
    "        global load_dataset_sample\n",
    "        \n",
    "        # Override the load_dataset_sample function temporarily\n",
    "        original_load_dataset = load_dataset_sample\n",
    "        load_dataset_sample = quick_test_sample\n",
    "        \n",
    "        # Override the prompt strategies temporarily\n",
    "        original_strategies = PROMPT_STRATEGIES\n",
    "        PROMPT_STRATEGIES = [\"zero_shot\"]\n",
    "        \n",
    "        try:\n",
    "            # Run evaluation with quick test settings\n",
    "            results = evaluate_models(dataset_path, sample_size, output_dir)\n",
    "        finally:\n",
    "            # Restore original functions\n",
    "            load_dataset_sample = original_load_dataset\n",
    "            PROMPT_STRATEGIES = original_strategies\n",
    "    else:\n",
    "        # Run normal evaluation\n",
    "        results = evaluate_models(dataset_path, sample_size, output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive framework for evaluating different ASL recognition models using various prompting strategies. The evaluation measures accuracy, precision, recall, and F1-score, and visualizes the results through confusion matrices and comparison charts.\n",
    "\n",
    "Key insights from such evaluations can help identify:\n",
    "- Which models perform best for ASL recognition\n",
    "- Which prompting strategies yield the best results for each model\n",
    "- Common misclassifications and error patterns\n",
    "- Efficiency metrics like token usage and response time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
